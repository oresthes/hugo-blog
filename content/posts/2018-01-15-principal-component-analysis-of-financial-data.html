---
title: Principal Component Analysis of Financial Data
author: Orest Alickolli
date: '2017-04-15'
categories:
  - R
tags:
  - PCA
  - TimeSeries
  - Linear Regression
slug: principal-component-analysis-of-financial-data
---



<p>Data from the past 30 years regarding yields of US Treasury bonds of multiple maturities have been collected. In this project, we deliberately explore the data to study correlation effects in linear predictors and then use PCA to reduce the dimensionality of the problem. We will be able to create through PCA a two-dimensional tableau and explore historical events in it.</p>
<div id="data" class="section level1">
<h1>Data</h1>
<pre><code>##           USGG3M USGG6M USGG2YR USGG3YR USGG5YR USGG10YR USGG30YR  Output1
## 1/5/1981   13.52  13.09  12.289   12.28  12.294   12.152   11.672 18.01553
## 1/6/1981   13.58  13.16  12.429   12.31  12.214   12.112   11.672 18.09140
## 1/7/1981   14.50  13.90  12.929   12.78  12.614   12.382   11.892 19.44731
## 1/8/1981   14.76  14.00  13.099   12.95  12.684   12.352   11.912 19.74851
## 1/9/1981   15.20  14.30  13.539   13.28  12.884   12.572   12.132 20.57204
## 1/12/1981  15.22  14.23  13.179   12.94  12.714   12.452   12.082 20.14218
##           Easing Tightening
## 1/5/1981      NA         NA
## 1/6/1981      NA         NA
## 1/7/1981      NA         NA
## 1/8/1981      NA         NA
## 1/9/1981      NA         NA
## 1/12/1981     NA         NA</code></pre>
<p>The first 7 variables (input variables) are the daily records of the US Treasury yields to maturity (%).<br />
Let’s observe the time trend.</p>
<pre class="r"><code>head(treasury_data[,1:7])</code></pre>
<pre><code>##           USGG3M USGG6M USGG2YR USGG3YR USGG5YR USGG10YR USGG30YR
## 1/5/1981   13.52  13.09  12.289   12.28  12.294   12.152   11.672
## 1/6/1981   13.58  13.16  12.429   12.31  12.214   12.112   11.672
## 1/7/1981   14.50  13.90  12.929   12.78  12.614   12.382   11.892
## 1/8/1981   14.76  14.00  13.099   12.95  12.684   12.352   11.912
## 1/9/1981   15.20  14.30  13.539   13.28  12.884   12.572   12.132
## 1/12/1981  15.22  14.23  13.179   12.94  12.714   12.452   12.082</code></pre>
<pre class="r"><code>matplot(AssignmentData[,-c(8,9,10)],type=&#39;l&#39;, xlab = &quot;Elapsed Trading Days&quot;, ylab = &quot;YTM(%)&quot;)</code></pre>
<p><img src="/posts/2018-01-15-principal-component-analysis-of-financial-data_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p>An unidentified variable is also present in the dataset. It is shown below plotted alongside the yield-to-maturity rates of the treasury bonds.</p>
<pre class="r"><code>matplot(treasury_data[,-c(9,10)],type=&#39;l&#39;, xlab = &quot;Elapsed Trading Days&quot;, ylab = &quot;YTM(%)&quot;)</code></pre>
<p><img src="/posts/2018-01-15-principal-component-analysis-of-financial-data_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
</div>
<div id="linear-regression-model-to-predict-output-using-ytm" class="section level1">
<h1>Linear Regression Model to predict Output using YTM</h1>
<p>Estimate simple regression model with each of the input variables and the output variable given in the dataset. Store coefficients for each input variable.</p>
<p>The following code gives an example of the analysis for the first input variable.</p>
<pre class="r"><code>Input1.linear.Model &lt;- lm(Output1~USGG3M, treasury_data)
Coefficients.Input1 &lt;- Input1.linear.Model$coefficients
summary(Input1.linear.Model)</code></pre>
<pre><code>## 
## Call:
## lm(formula = Output1 ~ USGG3M, data = treasury_data)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -6.9374 -1.2115 -0.0528  1.2640  7.7048 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -11.72318    0.03137  -373.7   &lt;2e-16 ***
## USGG3M        2.50756    0.00541   463.5   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.69 on 8298 degrees of freedom
## Multiple R-squared:  0.9628, Adjusted R-squared:  0.9628 
## F-statistic: 2.148e+05 on 1 and 8298 DF,  p-value: &lt; 2.2e-16</code></pre>
<pre class="r"><code>c(Total.Variance=var(AssignmentData[,8]),Unexplained.Variance=summary(Input1.linear.Model)$sigma^2)</code></pre>
<pre><code>##       Total.Variance Unexplained.Variance 
##            76.804438             2.857058</code></pre>
<pre class="r"><code>Coefficients.Input1</code></pre>
<pre><code>## (Intercept)      USGG3M 
##  -11.723184    2.507561</code></pre>
<pre class="r"><code>matplot(AssignmentData[,8],type=&quot;l&quot;,xaxt=&quot;n&quot;)
lines(Input1.linear.Model$fitted.values,col=&quot;red&quot;)</code></pre>
<p><img src="/posts/2018-01-15-principal-component-analysis-of-financial-data_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<div id="relevance-of-the-estimated-parameters-and-the-model-as-a-whole-amount-of-correlation-explained." class="section level2">
<h2>Relevance of the estimated parameters and the model as a whole, amount of correlation explained.</h2>
<p>The intercept and slope are both significant parameters in the linear regression model between the Output variable and the YTM for the 3M Treasury Bill.<br />
This is indicated by:<br />
* p-values - The p-values shown are assiocated with the t-test which evaluates the null hypothesis:<br />
<span class="math inline">\(H_0\)</span> : Parameter <span class="math inline">\(B_i\)</span> is equal to 0. Since both p-values are close to 0 and below any reasonable significance level, we can reject the null hypothesis and conclude that the two parameters are significant (different from 0.).<br />
* Standard errors - The standard errors calculated are both several orders of magnitude smaller than the parameter estimated. This confirms the alternate hypothesis that the parameters are different from 0.</p>
<p>R-squared indicates the amount of correlation explained by the linear model as a proportion of the total variance in the observed data (SSM/SST). The R-squared value for this regression model is 0.9628. This indicates a fairly high correlation between the predictor and the output.</p>
<p>An identical analysis can be performed on the other models.</p>
<p>Repeat fitting linear models with the same output Output1 and each of the inputs: USGG6M, USGG2YR, USGG3YR, USGG5YR, USGG10YR and USGG30YR.</p>
<p>Create the other models</p>
<pre class="r"><code># Create a list of all linear models. 
All.Linear.Models &lt;- apply(AssignmentData[,1:7], 2, function(x) lm(AssignmentData$Output1~x))</code></pre>
<p>Collect all slopes and intercepts in one table and print this table. Try to do it in one line using apply() function.</p>
<pre class="r"><code>All.Parameters &lt;- rbind.data.frame(lapply(All.Linear.Models, coefficients)) 
rownames(All.Parameters) &lt;- c(&quot;Intercept&quot;,&quot;Slope&quot;)
# Display Table of Results
All.Parameters</code></pre>
<pre><code>##               USGG3M     USGG6M    USGG2YR    USGG3YR    USGG5YR
## Intercept -11.723184 -12.097528 -13.055775 -13.861618 -15.436649
## Slope       2.507561   2.497235   2.400449   2.455793   2.568742
##             USGG10YR   USGG30YR
## Intercept -18.063370 -21.085905
## Slope       2.786991   3.069561</code></pre>
</div>
</div>
<div id="linear-regression-model-using-output-to-predict-yield" class="section level1">
<h1>Linear Regression model using Output to predict Yield</h1>
<p>Fit linear regression models using single output (column 8 Output1) as input and each of the original inputs as outputs. Collect all slopes and intercepts in one table and print this table.</p>
<pre class="r"><code>All.Linear.Models.Inverse &lt;- apply(AssignmentData[,1:7], 2, function(x) lm(x~AssignmentData$Output1))
All.Parameters.Inverse &lt;- rbind.data.frame(lapply(All.Linear.Models.Inverse, coefficients))
rownames(All.Parameters.Inverse) &lt;- c(&quot;Intercept&quot;,&quot;Slope&quot;)
# Display Table of Results
All.Parameters.Inverse</code></pre>
<pre><code>##              USGG3M   USGG6M   USGG2YR   USGG3YR  USGG5YR  USGG10YR
## Intercept 4.6751341 4.844370 5.4388879 5.6444580 6.009421 6.4813160
## Slope     0.3839609 0.390187 0.4151851 0.4063541 0.386061 0.3477544
##            USGG30YR
## Intercept 6.8693554
## Slope     0.3047124</code></pre>
</div>
<div id="logistic-regression-to-predict-tightening-and-easing-cycles." class="section level1">
<h1>Logistic Regression to predict tightening and easing cycles.</h1>
<p>Estimate logistic regression using all inputs and the data on FED tightening and easing cycles.</p>
<pre class="r"><code>AssignmentDataLogistic&lt;-data.matrix(AssignmentData,rownames.force=&quot;automatic&quot;)</code></pre>
<p>Prepare the easing-tightening data. Make the easing column equal to 0 during the easing periods and NA otherwise. Make the tightening column equal to 1 during the tightening periods and NA otherwise.</p>
<pre class="r"><code># Create columns of easing periods (as 0s) and tightening periods (as 1s)
EasingPeriods&lt;-AssignmentDataLogistic[,9]
EasingPeriods[AssignmentDataLogistic[,9]==1]&lt;-0
TighteningPeriods&lt;-AssignmentDataLogistic[,10]
# Check easing and tightening periods
cbind(EasingPeriods,TighteningPeriods)[c(550:560,900:910,970:980),]</code></pre>
<pre><code>##            EasingPeriods TighteningPeriods
## 3/29/1983             NA                NA
## 3/30/1983             NA                NA
## 3/31/1983             NA                NA
## 4/4/1983              NA                 1
## 4/5/1983              NA                 1
## 4/6/1983              NA                 1
## 4/7/1983              NA                 1
## 4/8/1983              NA                 1
## 4/11/1983             NA                 1
## 4/12/1983             NA                 1
## 4/13/1983             NA                 1
## 8/27/1984             NA                 1
## 8/28/1984             NA                 1
## 8/29/1984             NA                 1
## 8/30/1984             NA                 1
## 8/31/1984             NA                 1
## 9/4/1984              NA                NA
## 9/5/1984              NA                NA
## 9/6/1984              NA                NA
## 9/7/1984              NA                NA
## 9/10/1984             NA                NA
## 9/11/1984             NA                NA
## 12/10/1984             0                NA
## 12/11/1984             0                NA
## 12/12/1984             0                NA
## 12/13/1984             0                NA
## 12/14/1984             0                NA
## 12/17/1984             0                NA
## 12/18/1984             0                NA
## 12/19/1984             0                NA
## 12/20/1984             0                NA
## 12/21/1984             0                NA
## 12/24/1984             0                NA</code></pre>
<p>Remove the periods of neither easing nor tightening.</p>
<pre class="r"><code>All.NAs&lt;-is.na(EasingPeriods)&amp;is.na(TighteningPeriods)
AssignmentDataLogistic.EasingTighteningOnly&lt;-AssignmentDataLogistic
AssignmentDataLogistic.EasingTighteningOnly[,9]&lt;-EasingPeriods
AssignmentDataLogistic.EasingTighteningOnly&lt;-AssignmentDataLogistic.EasingTighteningOnly[!All.NAs,]
AssignmentDataLogistic.EasingTighteningOnly[is.na(AssignmentDataLogistic.EasingTighteningOnly[,10]),10]&lt;-0
# Binary output for logistic regression is now in column 10</code></pre>
<p>Plot the data and the binary output variable representing easing (0) and tightening (1) periods.</p>
<pre class="r"><code>matplot(AssignmentDataLogistic.EasingTighteningOnly[,-c(9,10)],type=&quot;l&quot;,ylab=&quot;Data and Binary Fed Mode&quot;)
lines(AssignmentDataLogistic.EasingTighteningOnly[,10]*20,col=&quot;red&quot;)</code></pre>
<p><img src="/posts/2018-01-15-principal-component-analysis-of-financial-data_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<p>Estimate logistic regression with 3M yields as predictors for easing/tightening output.</p>
<pre class="r"><code>LogisticModel.TighteningEasing_3M&lt;-glm(AssignmentDataLogistic.EasingTighteningOnly[,10]~
                                      AssignmentDataLogistic.EasingTighteningOnly[,1],family=binomial(link=logit))
summary(LogisticModel.TighteningEasing_3M)</code></pre>
<pre><code>## 
## Call:
## glm(formula = AssignmentDataLogistic.EasingTighteningOnly[, 10] ~ 
##     AssignmentDataLogistic.EasingTighteningOnly[, 1], family = binomial(link = logit))
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.4239  -0.9014  -0.7737   1.3548   1.6743  
## 
## Coefficients:
##                                                  Estimate Std. Error
## (Intercept)                                      -2.15256    0.17328
## AssignmentDataLogistic.EasingTighteningOnly[, 1]  0.18638    0.02144
##                                                  z value Pr(&gt;|z|)    
## (Intercept)                                      -12.422   &lt;2e-16 ***
## AssignmentDataLogistic.EasingTighteningOnly[, 1]   8.694   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 2983.5  on 2357  degrees of freedom
## Residual deviance: 2904.8  on 2356  degrees of freedom
## AIC: 2908.8
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<pre class="r"><code>matplot(AssignmentDataLogistic.EasingTighteningOnly[,-c(9,10)],type=&quot;l&quot;,ylab=&quot;Data and Fitted Values&quot;)
lines(AssignmentDataLogistic.EasingTighteningOnly[,10]*20,col=&quot;red&quot;)
lines(LogisticModel.TighteningEasing_3M$fitted.values*20,col=&quot;green&quot;)</code></pre>
<p><img src="/posts/2018-01-15-principal-component-analysis-of-financial-data_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<p>Now use all inputs as predictors for logistic regression.</p>
<pre class="r"><code>LogisticModel.TighteningEasing_All&lt;- glm(Tightening~.,family=binomial(link=logit), data=data.frame(AssignmentDataLogistic.EasingTighteningOnly)[,-c(8,9)])
summary(LogisticModel.TighteningEasing_All)$aic</code></pre>
<pre><code>## [1] 2645.579</code></pre>
<pre class="r"><code>summary(LogisticModel.TighteningEasing_All)$coefficients[,c(1,4)]</code></pre>
<pre><code>##               Estimate     Pr(&gt;|z|)
## (Intercept) -4.7551928 2.784283e-28
## USGG3M      -3.3456116 4.073045e-36
## USGG6M       4.1558535 1.422964e-28
## USGG2YR      3.9460296 1.751687e-07
## USGG3YR     -3.4642455 2.080617e-04
## USGG5YR     -3.2115319 3.786229e-05
## USGG10YR    -0.9705444 3.202140e-01
## USGG30YR     3.3253517 6.036041e-08</code></pre>
<pre class="r"><code># Plot
matplot(AssignmentDataLogistic.EasingTighteningOnly[,-c(9,10)],type=&quot;l&quot;,ylab=&quot;Results of Logistic Regression&quot;)
lines(AssignmentDataLogistic.EasingTighteningOnly[,10]*20,col=&quot;red&quot;)
lines(LogisticModel.TighteningEasing_All$fitted.values*20,col=&quot;green&quot;)</code></pre>
<p><img src="/posts/2018-01-15-principal-component-analysis-of-financial-data_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
<p>Interpret the coefficients of the model and the fitted values.</p>
<p><strong>Answer:</strong> This logistic regression model has as predictors a subset of the US Treasury YTM for various maturities. The response is a categorical variable with two levels (1 and 0/succes and failure) representing respectively the application or not of tightening policies by the FED.<br />
The idea behind the logistic regression model is to use the predictors to estimate the probability of success (i.e of tightening policies in our case).</p>
<p><span class="math display">\[ \pi(x) = \beta_0 + \sum_{i=1}^{7} \beta_i x_i + \epsilon \]</span></p>
<p>However in the model above , <span class="math inline">\(\pi(x)\)</span> has a different range from its right-hand-side. Therefore the logit transformation is used:</p>
<p><span class="math display">\[ \log(\frac{\pi(x)}{1-\pi(x)}) = \beta_0 + \sum_{i=1}^{7} \beta_i x_i + \epsilon \]</span></p>
<p>where the left hand side now represents the log odds. The <em>log odds</em> are the fitted values of this model and are shown plotted in the graph below. From the log odds we can easily obtain the associated <em>probabilities</em>, which are also shown plotted.</p>
<p>The coefficients of the model (<span class="math inline">\(\beta\)</span>) have the following interpreation.</p>
<p><span class="math display">\[ log(\frac{\pi(x+1)}{1-\pi(x+1)}) - log(\frac{\pi(x)}{1-\pi(x)}) = \beta_0 + \sum_{i=1}^{7} \beta_i(x_i+1) - \beta_0 - \sum_{i=1}^{7} \beta_ix_i\]</span></p>
<p><span class="math display">\[\ log( \frac{\frac{\pi(x+1)}{1-\pi(x+1)} }{ \frac{\pi(x)}{1-\pi(x)} }) = \sum_{i=1}^{7} \beta_i  \]</span></p>
<p>and therefore <span class="math inline">\(e^{\sum\beta_i} = e^{\beta_1} e^{\beta_2} ... e^{\beta_7}\)</span> is equal to the odds ratio. This signifies for each increase of 1 unit in the predictor variable <span class="math inline">\(x_i\)</span>, the odds will increase or decrease by a factor of <span class="math inline">\(e^{\beta_i}\)</span> (depending on the sign of <span class="math inline">\(\beta\)</span>).</p>
<p>For our current application, it signifies that an increase in YTM of instruments associated with positive <span class="math inline">\(\beta&#39;s\)</span> (6M,2YR and 10YR) will lead to an increase in odds, while and increase in YTM of instruments associated with negative <span class="math inline">\(\beta&#39;s\)</span> (3M,3YR,5YR,10YR) will lead to a decrease in odds (where odds = <span class="math inline">\(\pi/(1-\pi))\)</span>).</p>
<p>An interesting note at this point relates to the parameter associated with the USGG3M bond. When the model was fit using only the USGG3M bond, its effect on predicting easing and tightening cycles was the opposite we notice in the full model (associated <span class="math inline">\(\beta\)</span> changed from 0.186 to -3.345).<br />
I presume this is due to the fact that the YTM values are related to each other and their effect cannot be considered as purely additive.</p>
<p>Calculate and plot log-odds and probabilities.</p>
<pre class="r"><code># Calculate odds
Log.Odds&lt;-predict(LogisticModel.TighteningEasing_All)
plot(Log.Odds,type=&quot;l&quot;)</code></pre>
<p><img src="/posts/2018-01-15-principal-component-analysis-of-financial-data_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
<pre class="r"><code>Probabilities&lt;-1/(exp(-Log.Odds)+1)
plot(LogisticModel.TighteningEasing_All$fitted.values,type=&quot;l&quot;,ylab=&quot;Fitted Values &amp; Log-Odds&quot;)
lines(Probabilities,col=&quot;red&quot;)</code></pre>
<p><img src="/posts/2018-01-15-principal-component-analysis-of-financial-data_files/figure-html/unnamed-chunk-15-2.png" width="672" /></p>
<p>Compare probabilities with fitted values.</p>
<p><strong>Answer:</strong> Negative log-odds values correspond to odds of less than 1 and positive log-odds correspond to odds higher than 1.<br />
Odds less than 1 will correspond to a probability of less than 0.5, while odds higher than 1 will correspond to probabilities higher than 0.5.</p>
</div>
<div id="step-5" class="section level1">
<h1>Step 5</h1>
<p>Compare linear regression models with different combinations of predictors. Select the best combination.</p>
<p>Below we show only two of possible combinations: full model containing all 7 predictors and Null model containing only intercept, but none of the 7 predictors. Estimate other possible combinations.</p>
<pre class="r"><code>AssignmentDataRegressionComparison&lt;-AssignmentData[,-c(9,10)]</code></pre>
<pre class="r"><code>RegressionModelComparison.Full &lt;- lm(Output1~.,data=AssignmentDataRegressionComparison)</code></pre>
<p>Look at coefficients, <span class="math inline">\(R^2\)</span>, adjusted <span class="math inline">\(R2\)</span>, degrees of freedom.</p>
<ol style="list-style-type: decimal">
<li>Coefficients</li>
</ol>
<pre class="r"><code>summary(RegressionModelComparison.Full)$coefficients</code></pre>
<pre><code>##                Estimate   Std. Error       t value Pr(&gt;|t|)
## (Intercept) -14.9041591 1.056850e-10 -141024294891        0
## USGG3M        0.3839609 9.860401e-11    3893968285        0
## USGG6M        0.3901870 1.500111e-10    2601053702        0
## USGG2YR       0.4151851 2.569451e-10    1615851177        0
## USGG3YR       0.4063541 3.299038e-10    1231735395        0
## USGG5YR       0.3860610 2.618339e-10    1474449865        0
## USGG10YR      0.3477544 2.800269e-10    1241860763        0
## USGG30YR      0.3047124 1.566487e-10    1945195584        0</code></pre>
<ol start="2" style="list-style-type: decimal">
<li><span class="math inline">\(R^2\)</span> and <span class="math inline">\(R^2\)</span>-adjusted.</li>
</ol>
<pre class="r"><code>summary(RegressionModelComparison.Full)$r.squared</code></pre>
<pre><code>## [1] 1</code></pre>
<pre class="r"><code>summary(RegressionModelComparison.Full)$adj.r.squared</code></pre>
<pre><code>## [1] 1</code></pre>
<ol start="3" style="list-style-type: decimal">
<li>Degrees of freedom</li>
</ol>
<pre class="r"><code>summary(RegressionModelComparison.Full)$df</code></pre>
<pre><code>## [1]    8 8292    8</code></pre>
<p>Intepret the fitted model. How good is the fit? How significant are the parameters?</p>
<p><strong>Answer:</strong> The fit is perfect with an R-squared of 1 and a virtually 0 residual standard error. All parameters are significant with p-values close to 0 and standard errors several orders of mangnitude lower than the estimated parameters.</p>
<p>Estimate the Null model by including only intercept.</p>
<pre class="r"><code>RegressionModelComparison.Null &lt;- lm(Output1~1,data=AssignmentDataRegressionComparison)</code></pre>
<p>Explore the same parameters of the fitted null model as for the full model.</p>
<pre class="r"><code>summary(RegressionModelComparison.Null)$coefficients</code></pre>
<pre><code>##                 Estimate Std. Error      t value Pr(&gt;|t|)
## (Intercept) 1.420082e-11 0.09619536 1.476248e-10        1</code></pre>
<pre class="r"><code>summary(RegressionModelComparison.Null)$r.squared</code></pre>
<pre><code>## [1] 0</code></pre>
<pre class="r"><code>summary(RegressionModelComparison.Null)$adj.r.squared</code></pre>
<pre><code>## [1] 0</code></pre>
<pre class="r"><code>summary(RegressionModelComparison.Null)$df</code></pre>
<pre><code>## [1]    1 8299    1</code></pre>
<p>Why <code>summary(RegressionModelComparison.Null)</code> does not show <span class="math inline">\(R^2\)</span>?</p>
<p><strong>Answer:</strong> By definition R is the ratio of the effect variance over the total variance. In this case we do not have effect variance since there are no predictors associated with this model. Therefore SST and SSE are equal to each other (considering the mean as the fitted model).</p>
<p><span class="math display">\[ R^2 = \frac{1-SSE}{SST} = 1 - 1 = 0\]</span></p>
<p>Compare models pairwise using <code>anova()</code></p>
<pre class="r"><code>anova(RegressionModelComparison.Full,RegressionModelComparison.Null)</code></pre>
<pre><code>## Analysis of Variance Table
## 
## Model 1: Output1 ~ USGG3M + USGG6M + USGG2YR + USGG3YR + USGG5YR + USGG10YR + 
##     USGG30YR
## Model 2: Output1 ~ 1
##   Res.Df    RSS Df Sum of Sq        F    Pr(&gt;F)    
## 1   8292      0                                    
## 2   8299 637400 -7   -637400 3.73e+22 &lt; 2.2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>Interpret the results of anova().</p>
<p><strong>Answer:</strong> The test shown in the output of this model comparison ANOVA, is also known as the model utility test. The two models being compared are:</p>
<p><span class="math display">\[\Omega: E(Y) = \beta_0 + \beta_1 x_1 + ... + \beta_7 x_7\]</span> <span class="math display">\[\omega: E(Y) = \beta_0\]</span></p>
<p>with the null hypothesis being</p>
<p><span class="math display">\[H_0 : \beta_i = 0\]</span></p>
<p>where i = 1 to 7.</p>
<p>Since the F-value of the test is much higher than 1 and the p-value is practically 0, we can reject the null hypothesis stated above. Therefore we can say that at least one of the <span class="math inline">\(\beta\)</span> parameters is different than 0 and that the larger model is a superior fit (much superior judging from the reduction in RSS).</p>
<p>Repeat the analysis for different combinations of input variables and select the one you think is the best. Explain your selection.</p>
<p>A stepwise regression by dropping single terms will be used to see how removing some parameters affects the fit, and if we can simplify the model.</p>
<pre class="r"><code>step(RegressionModelComparison.Full)</code></pre>
<pre><code>## Start:  AIC=-336590.5
## Output1 ~ USGG3M + USGG6M + USGG2YR + USGG3YR + USGG5YR + USGG10YR + 
##     USGG30YR</code></pre>
<pre><code>## Warning: attempting model selection on an essentially perfect fit is
## nonsense</code></pre>
<pre><code>##            Df Sum of Sq    RSS     AIC
## &lt;none&gt;                   0.000 -336591
## - USGG3YR   1     3.704  3.704  -64018
## - USGG10YR  1     3.765  3.765  -63882
## - USGG5YR   1     5.307  5.307  -61032
## - USGG2YR   1     6.374  6.374  -59512
## - USGG30YR  1     9.237  9.237  -56433
## - USGG6M    1    16.516 16.516  -51609
## - USGG3M    1    37.016 37.016  -44911</code></pre>
<pre><code>## 
## Call:
## lm(formula = Output1 ~ USGG3M + USGG6M + USGG2YR + USGG3YR + 
##     USGG5YR + USGG10YR + USGG30YR, data = AssignmentDataRegressionComparison)
## 
## Coefficients:
## (Intercept)       USGG3M       USGG6M      USGG2YR      USGG3YR  
##    -14.9042       0.3840       0.3902       0.4152       0.4064  
##     USGG5YR     USGG10YR     USGG30YR  
##      0.3861       0.3478       0.3047</code></pre>
<p>The warning message says that the model cannot be improved, since we already have a perfect fit. We can manually try by using the drop1() and add1() functions to find a simpler model.</p>
<pre class="r"><code>drop1(RegressionModelComparison.Full)</code></pre>
<pre><code>## Warning: attempting model selection on an essentially perfect fit is
## nonsense</code></pre>
<pre><code>## Single term deletions
## 
## Model:
## Output1 ~ USGG3M + USGG6M + USGG2YR + USGG3YR + USGG5YR + USGG10YR + 
##     USGG30YR
##          Df Sum of Sq    RSS     AIC
## &lt;none&gt;                 0.000 -336591
## USGG3M    1    37.016 37.016  -44911
## USGG6M    1    16.516 16.516  -51609
## USGG2YR   1     6.374  6.374  -59512
## USGG3YR   1     3.704  3.704  -64018
## USGG5YR   1     5.307  5.307  -61032
## USGG10YR  1     3.765  3.765  -63882
## USGG30YR  1     9.237  9.237  -56433</code></pre>
<p>Remove USGG3YR</p>
<pre class="r"><code>RegressionModelComparison.Reduced1 &lt;- lm(Output1~.,data=AssignmentDataRegressionComparison[,-4])
summary(RegressionModelComparison.Reduced1)</code></pre>
<pre><code>## 
## Call:
## lm(formula = Output1 ~ ., data = AssignmentDataRegressionComparison[, 
##     -4])
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.36972 -0.01132 -0.00141  0.00905  0.30202 
## 
## Coefficients:
##               Estimate Std. Error   t value Pr(&gt;|t|)    
## (Intercept) -14.863604   0.001358 -10942.56   &lt;2e-16 ***
## USGG3M        0.389687   0.001332    292.51   &lt;2e-16 ***
## USGG6M        0.357731   0.001997    179.09   &lt;2e-16 ***
## USGG2YR       0.685589   0.001806    379.63   &lt;2e-16 ***
## USGG5YR       0.577177   0.002853    202.33   &lt;2e-16 ***
## USGG10YR      0.347515   0.003788     91.75   &lt;2e-16 ***
## USGG30YR      0.270634   0.002085    129.77   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.02113 on 8293 degrees of freedom
## Multiple R-squared:      1,  Adjusted R-squared:      1 
## F-statistic: 2.379e+08 on 6 and 8293 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>We still have a good fit, so we can proceed with dropping a second term</p>
<pre class="r"><code>drop1(RegressionModelComparison.Reduced1)</code></pre>
<pre><code>## Single term deletions
## 
## Model:
## Output1 ~ USGG3M + USGG6M + USGG2YR + USGG5YR + USGG10YR + USGG30YR
##          Df Sum of Sq    RSS    AIC
## &lt;none&gt;                 3.704 -64018
## USGG3M    1    38.213 41.917 -43881
## USGG6M    1    14.325 18.028 -50884
## USGG2YR   1    64.366 68.070 -39857
## USGG5YR   1    18.282 21.986 -49237
## USGG10YR  1     3.760  7.463 -58204
## USGG30YR  1     7.521 11.225 -54817</code></pre>
<p>Remove USGG10YR</p>
<pre class="r"><code>RegressionModelComparison.Reduced2 &lt;- lm(Output1~.,data=AssignmentDataRegressionComparison[,-c(6,4)])
summary(RegressionModelComparison.Reduced2)</code></pre>
<pre><code>## 
## Call:
## lm(formula = Output1 ~ ., data = AssignmentDataRegressionComparison[, 
##     -c(6, 4)])
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.37829 -0.01722 -0.00095  0.01724  0.43850 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -14.908651   0.001798 -8293.0   &lt;2e-16 ***
## USGG3M        0.383598   0.001889   203.1   &lt;2e-16 ***
## USGG6M        0.366675   0.002832   129.5   &lt;2e-16 ***
## USGG2YR       0.644176   0.002482   259.5   &lt;2e-16 ***
## USGG5YR       0.791796   0.002318   341.6   &lt;2e-16 ***
## USGG30YR      0.447951   0.001113   402.6   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.03 on 8294 degrees of freedom
## Multiple R-squared:      1,  Adjusted R-squared:      1 
## F-statistic: 1.417e+08 on 5 and 8294 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>We still have an almost perfect fit. We can remove an additional term.</p>
<pre class="r"><code>drop1(RegressionModelComparison.Reduced2)</code></pre>
<pre><code>## Single term deletions
## 
## Model:
## Output1 ~ USGG3M + USGG6M + USGG2YR + USGG5YR + USGG30YR
##          Df Sum of Sq     RSS    AIC
## &lt;none&gt;                  7.463 -58204
## USGG3M    1    37.121  44.584 -43371
## USGG6M    1    15.086  22.549 -49029
## USGG2YR   1    60.611  68.074 -39858
## USGG5YR   1   105.012 112.476 -35691
## USGG30YR  1   145.865 153.329 -33119</code></pre>
<p>Remove USGG6M</p>
<pre class="r"><code>RegressionModelComparison.Reduced3 &lt;- lm(Output1~.,data=AssignmentDataRegressionComparison[,-c(2,6,4)])
summary(RegressionModelComparison.Reduced3)</code></pre>
<pre><code>## 
## Call:
## lm(formula = Output1 ~ ., data = AssignmentDataRegressionComparison[, 
##     -c(2, 6, 4)])
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.38697 -0.03049 -0.00084  0.03257  0.43021 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -14.812574   0.002846 -5204.6   &lt;2e-16 ***
## USGG3M        0.607302   0.001326   458.0   &lt;2e-16 ***
## USGG2YR       0.843754   0.003381   249.5   &lt;2e-16 ***
## USGG5YR       0.744117   0.003977   187.1   &lt;2e-16 ***
## USGG30YR      0.423994   0.001907   222.3   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.05214 on 8295 degrees of freedom
## Multiple R-squared:      1,  Adjusted R-squared:      1 
## F-statistic: 5.862e+07 on 4 and 8295 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>Remove additional term.</p>
<pre class="r"><code>drop1(RegressionModelComparison.Reduced3)</code></pre>
<pre><code>## Single term deletions
## 
## Model:
## Output1 ~ USGG3M + USGG2YR + USGG5YR + USGG30YR
##          Df Sum of Sq    RSS    AIC
## &lt;none&gt;                 22.55 -49029
## USGG3M    1    570.22 592.77 -21897
## USGG2YR   1    169.26 191.81 -31262
## USGG5YR   1     95.15 117.70 -35316
## USGG30YR  1    134.40 156.95 -32927</code></pre>
<p>Remove USGG5YR</p>
<pre class="r"><code>RegressionModelComparison.Reduced4 &lt;- lm(Output1~.,data=AssignmentDataRegressionComparison[,-c(5,2,6,4)])
summary(RegressionModelComparison.Reduced4)</code></pre>
<pre><code>## 
## Call:
## lm(formula = Output1 ~ ., data = AssignmentDataRegressionComparison[, 
##     -c(5, 2, 6, 4)])
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.55605 -0.05109  0.01731  0.08219  0.70527 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -15.055581   0.005785 -2602.5   &lt;2e-16 ***
## USGG3M        0.496988   0.002713   183.2   &lt;2e-16 ***
## USGG2YR       1.404153   0.003584   391.8   &lt;2e-16 ***
## USGG30YR      0.741709   0.001981   374.4   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.1191 on 8296 degrees of freedom
## Multiple R-squared:  0.9998, Adjusted R-squared:  0.9998 
## F-statistic: 1.497e+07 on 3 and 8296 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>We are starting to notice some reduction in the value of R-squared. However, we can attempt to drop an additional predictor.</p>
<pre class="r"><code>drop1(RegressionModelComparison.Reduced4)</code></pre>
<pre><code>## Single term deletions
## 
## Model:
## Output1 ~ USGG3M + USGG2YR + USGG30YR
##          Df Sum of Sq    RSS    AIC
## &lt;none&gt;                 117.7 -35316
## USGG3M    1     476.0  593.7 -21886
## USGG2YR   1    2177.7 2295.4 -10662
## USGG30YR  1    1988.2 2105.9 -11378</code></pre>
<p>Remove USGG3M</p>
<pre class="r"><code>RegressionModelComparison.Reduced5 &lt;- lm(Output1~.,data=AssignmentDataRegressionComparison[,-c(1,5,2,6,4)])
summary(RegressionModelComparison.Reduced5)</code></pre>
<pre><code>## 
## Call:
## lm(formula = Output1 ~ ., data = AssignmentDataRegressionComparison[, 
##     -c(1, 5, 2, 6, 4)])
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -1.65662 -0.17773  0.01775  0.17526  1.84678 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -14.590584   0.011675 -1249.8   &lt;2e-16 ***
## USGG2YR       2.023026   0.002685   753.5   &lt;2e-16 ***
## USGG30YR      0.522257   0.003544   147.4   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.2675 on 8297 degrees of freedom
## Multiple R-squared:  0.9991, Adjusted R-squared:  0.9991 
## F-statistic: 4.45e+06 on 2 and 8297 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>We can see that even with just two predictors left, we still have a very good fit. For completeness lets try to fit the model with just one predictor</p>
<pre class="r"><code>drop1(RegressionModelComparison.Reduced5)</code></pre>
<pre><code>## Single term deletions
## 
## Model:
## Output1 ~ USGG2YR + USGG30YR
##          Df Sum of Sq   RSS    AIC
## &lt;none&gt;                  594 -21886
## USGG2YR   1     40625 41219  13306
## USGG30YR  1      1554  2148 -11217</code></pre>
<p>Remove USGG30YR</p>
<pre class="r"><code>RegressionModelComparison.Reduced6 &lt;- lm(Output1~.,data=AssignmentDataRegressionComparison[,-c(7,1,5,2,6,4)])
summary(RegressionModelComparison.Reduced6)</code></pre>
<pre><code>## 
## Call:
## lm(formula = Output1 ~ ., data = AssignmentDataRegressionComparison[, 
##     -c(7, 1, 5, 2, 6, 4)])
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -1.43277 -0.38356 -0.00578  0.43362  1.72564 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -13.055775   0.010031   -1302   &lt;2e-16 ***
## USGG2YR       2.400449   0.001532    1567   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.5087 on 8298 degrees of freedom
## Multiple R-squared:  0.9966, Adjusted R-squared:  0.9966 
## F-statistic: 2.455e+06 on 1 and 8298 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>We can see that even a single predictor gives a model with a very high R squared. Checking the R-squared values of all single predictor models we can see that even though there are minor variations, all single predictor models provide a very satisfactory fit. This is due to the fact that there is strong correlation between the all the models and each individual predictor can capture well the variation in the other predictors.</p>
<pre class="r"><code>lapply(All.Linear.Models,function(x) summary(x)$r.squared)</code></pre>
<pre><code>## $USGG3M
## [1] 0.9628054
## 
## $USGG6M
## [1] 0.9743884
## 
## $USGG2YR
## [1] 0.9966307
## 
## $USGG3YR
## [1] 0.9979215
## 
## $USGG5YR
## [1] 0.9916908
## 
## $USGG10YR
## [1] 0.9691884
## 
## $USGG30YR
## [1] 0.9353333</code></pre>
<pre class="r"><code>cor(AssignmentData[,1:7])</code></pre>
<pre><code>##             USGG3M    USGG6M   USGG2YR   USGG3YR   USGG5YR  USGG10YR
## USGG3M   1.0000000 0.9979328 0.9843135 0.9768273 0.9603153 0.9348717
## USGG6M   0.9979328 1.0000000 0.9910013 0.9845272 0.9691905 0.9444964
## USGG2YR  0.9843135 0.9910013 1.0000000 0.9987920 0.9916366 0.9753836
## USGG3YR  0.9768273 0.9845272 0.9987920 1.0000000 0.9963859 0.9836936
## USGG5YR  0.9603153 0.9691905 0.9916366 0.9963859 1.0000000 0.9948318
## USGG10YR 0.9348717 0.9444964 0.9753836 0.9836936 0.9948318 1.0000000
## USGG30YR 0.9069437 0.9167172 0.9539191 0.9648236 0.9819750 0.9956347
##           USGG30YR
## USGG3M   0.9069437
## USGG6M   0.9167172
## USGG2YR  0.9539191
## USGG3YR  0.9648236
## USGG5YR  0.9819750
## USGG10YR 0.9956347
## USGG30YR 1.0000000</code></pre>
</div>
<div id="step-6" class="section level1">
<h1>Step 6</h1>
<p>Perform rolling window analysis of the yields data.<br />
Use package <code>zoo</code> for rolling window analysis.<br />
Set the window width and window shift parameters for rolling window.</p>
<pre class="r"><code>Window.width&lt;-20 
Window.shift&lt;-5</code></pre>
<p>Run rolling mean values using <code>rollapply()</code>.</p>
<pre class="r"><code>library(zoo)</code></pre>
<pre><code>## 
## Attaching package: &#39;zoo&#39;</code></pre>
<pre><code>## The following objects are masked from &#39;package:base&#39;:
## 
##     as.Date, as.Date.numeric</code></pre>
<p>Calculate rolling mean values for each variable.</p>
<pre class="r"><code># Means
all.means&lt;-rollapply(AssignmentDataRegressionComparison,width=Window.width,by=Window.shift,by.column=TRUE, mean)
head(all.means,10)</code></pre>
<pre><code>##        USGG3M  USGG6M USGG2YR USGG3YR USGG5YR USGG10YR USGG30YR  Output1
##  [1,] 15.0405 14.0855 13.2795 12.9360 12.7825  12.5780  12.1515 20.14842
##  [2,] 15.1865 14.1440 13.4855 13.1085 12.9310  12.7370  12.3370 20.55208
##  [3,] 15.2480 14.2755 13.7395 13.3390 13.1500  12.9480  12.5500 21.04895
##  [4,] 14.9345 14.0780 13.7750 13.4765 13.2385  13.0515  12.6610 21.02611
##  [5,] 14.7545 14.0585 13.9625 13.6890 13.4600  13.2295  12.8335 21.31356
##  [6,] 14.6025 14.0115 14.0380 13.7790 13.5705  13.3050  12.8890 21.39061
##  [7,] 14.0820 13.5195 13.8685 13.6710 13.4815  13.1880  12.7660 20.77200
##  [8,] 13.6255 13.0635 13.6790 13.5735 13.4270  13.1260  12.6950 20.23626
##  [9,] 13.2490 12.6810 13.5080 13.4575 13.3680  13.0770  12.6470 19.76988
## [10,] 12.9545 12.4225 13.4140 13.4240 13.3850  13.1115  12.6755 19.53054</code></pre>
<pre class="r"><code># Create points at which rolling means are calculated
Count&lt;-1:length(AssignmentDataRegressionComparison[,1])
Rolling.window.matrix&lt;-rollapply(Count,width=Window.width,by=Window.shift,by.column=FALSE,FUN=function(z) z)
Rolling.window.matrix[1:10,]</code></pre>
<pre><code>##       [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13]
##  [1,]    1    2    3    4    5    6    7    8    9    10    11    12    13
##  [2,]    6    7    8    9   10   11   12   13   14    15    16    17    18
##  [3,]   11   12   13   14   15   16   17   18   19    20    21    22    23
##  [4,]   16   17   18   19   20   21   22   23   24    25    26    27    28
##  [5,]   21   22   23   24   25   26   27   28   29    30    31    32    33
##  [6,]   26   27   28   29   30   31   32   33   34    35    36    37    38
##  [7,]   31   32   33   34   35   36   37   38   39    40    41    42    43
##  [8,]   36   37   38   39   40   41   42   43   44    45    46    47    48
##  [9,]   41   42   43   44   45   46   47   48   49    50    51    52    53
## [10,]   46   47   48   49   50   51   52   53   54    55    56    57    58
##       [,14] [,15] [,16] [,17] [,18] [,19] [,20]
##  [1,]    14    15    16    17    18    19    20
##  [2,]    19    20    21    22    23    24    25
##  [3,]    24    25    26    27    28    29    30
##  [4,]    29    30    31    32    33    34    35
##  [5,]    34    35    36    37    38    39    40
##  [6,]    39    40    41    42    43    44    45
##  [7,]    44    45    46    47    48    49    50
##  [8,]    49    50    51    52    53    54    55
##  [9,]    54    55    56    57    58    59    60
## [10,]    59    60    61    62    63    64    65</code></pre>
<pre class="r"><code># Take middle of each window
Points.of.calculation&lt;-Rolling.window.matrix[,10]
Points.of.calculation[1:10]</code></pre>
<pre><code>##  [1] 10 15 20 25 30 35 40 45 50 55</code></pre>
<pre class="r"><code>length(Points.of.calculation)</code></pre>
<pre><code>## [1] 1657</code></pre>
<pre class="r"><code># Insert means into the total length vector to plot the rolling mean with the original data
Means.forPlot&lt;-rep(NA,length(AssignmentDataRegressionComparison[,1]))
Means.forPlot[Points.of.calculation]&lt;-all.means[,1]
Means.forPlot[1:50]</code></pre>
<pre><code>##  [1]      NA      NA      NA      NA      NA      NA      NA      NA
##  [9]      NA 15.0405      NA      NA      NA      NA 15.1865      NA
## [17]      NA      NA      NA 15.2480      NA      NA      NA      NA
## [25] 14.9345      NA      NA      NA      NA 14.7545      NA      NA
## [33]      NA      NA 14.6025      NA      NA      NA      NA 14.0820
## [41]      NA      NA      NA      NA 13.6255      NA      NA      NA
## [49]      NA 13.2490</code></pre>
<pre class="r"><code># Assemble the matrix to plot the rolling means
cbind(AssignmentDataRegressionComparison[,1],Means.forPlot)[1:50,]</code></pre>
<pre><code>##             Means.forPlot
##  [1,] 13.52            NA
##  [2,] 13.58            NA
##  [3,] 14.50            NA
##  [4,] 14.76            NA
##  [5,] 15.20            NA
##  [6,] 15.22            NA
##  [7,] 15.24            NA
##  [8,] 15.08            NA
##  [9,] 15.25            NA
## [10,] 15.15       15.0405
## [11,] 15.79            NA
## [12,] 15.32            NA
## [13,] 15.71            NA
## [14,] 15.72            NA
## [15,] 15.70       15.1865
## [16,] 15.35            NA
## [17,] 15.20            NA
## [18,] 15.06            NA
## [19,] 14.87            NA
## [20,] 14.59       15.2480
## [21,] 14.90            NA
## [22,] 14.85            NA
## [23,] 14.67            NA
## [24,] 14.74            NA
## [25,] 15.32       14.9345
## [26,] 15.52            NA
## [27,] 15.46            NA
## [28,] 15.54            NA
## [29,] 15.51            NA
## [30,] 15.14       14.7545
## [31,] 15.02            NA
## [32,] 14.48            NA
## [33,] 14.09            NA
## [34,] 14.23            NA
## [35,] 14.15       14.6025
## [36,] 14.20            NA
## [37,] 14.14            NA
## [38,] 14.22            NA
## [39,] 14.52            NA
## [40,] 14.39       14.0820
## [41,] 14.49            NA
## [42,] 14.51            NA
## [43,] 14.29            NA
## [44,] 14.16            NA
## [45,] 13.99       13.6255
## [46,] 13.92            NA
## [47,] 13.66            NA
## [48,] 13.21            NA
## [49,] 13.02            NA
## [50,] 12.95       13.2490</code></pre>
<pre class="r"><code>plot(Means.forPlot,col=&quot;red&quot;)
lines(AssignmentDataRegressionComparison[,1])</code></pre>
<p><img src="/posts/2018-01-15-principal-component-analysis-of-financial-data_files/figure-html/unnamed-chunk-40-1.png" width="672" /></p>
<p>Run rolling daily difference standard deviation of each variable</p>
<pre class="r"><code># Obtain data frame of daily differences
DailyDifferenceData &lt;- AssignmentDataRegressionComparison[-1,] - head(AssignmentDataRegressionComparison,-1)
head(DailyDifferenceData)</code></pre>
<pre><code>##           USGG3M USGG6M USGG2YR USGG3YR USGG5YR USGG10YR USGG30YR
## 1/6/1981    0.06   0.07    0.14    0.03   -0.08    -0.04     0.00
## 1/7/1981    0.92   0.74    0.50    0.47    0.40     0.27     0.22
## 1/8/1981    0.26   0.10    0.17    0.17    0.07    -0.03     0.02
## 1/9/1981    0.44   0.30    0.44    0.33    0.20     0.22     0.22
## 1/12/1981   0.02  -0.07   -0.36   -0.34   -0.17    -0.12    -0.05
## 1/13/1981   0.02  -0.13    0.13    0.03   -0.03     0.08     0.00
##               Output1
## 1/6/1981   0.07587222
## 1/7/1981   1.35591615
## 1/8/1981   0.30119608
## 1/9/1981   0.82353207
## 1/12/1981 -0.42985741
## 1/13/1981  0.03935812</code></pre>
<pre class="r"><code># Apply rolling function to daily differences
rolling.sd &lt;-rollapply(DailyDifferenceData,width=Window.width,by=Window.shift,by.column=TRUE, FUN=sd)
head(rolling.sd)</code></pre>
<pre><code>##         USGG3M    USGG6M   USGG2YR   USGG3YR   USGG5YR  USGG10YR  USGG30YR
## [1,] 0.3433258 0.3262462 0.2748258 0.2030161 0.1713192 0.1299585 0.1202147
## [2,] 0.2933383 0.2907504 0.2261811 0.1499219 0.1450082 0.1146895 0.1192201
## [3,] 0.2613180 0.2437530 0.2006201 0.1632596 0.1654110 0.1459308 0.1351909
## [4,] 0.2551754 0.2469663 0.1989446 0.1692794 0.1717219 0.1551052 0.1422183
## [5,] 0.2480551 0.2481595 0.2102004 0.1786057 0.1744767 0.1643960 0.1516540
## [6,] 0.1963884 0.2363672 0.2095082 0.1809180 0.1822917 0.1664956 0.1537351
##        Output1
## [1,] 0.5639875
## [2,] 0.4707427
## [3,] 0.4681168
## [4,] 0.4786189
## [5,] 0.4888569
## [6,] 0.4788897</code></pre>
<pre class="r"><code># Identidy rolling dates
rolling.dates&lt;-rollapply(AssignmentDataRegressionComparison[-1,],width=Window.width,by=Window.shift,
                         by.column=FALSE,FUN=function(z) rownames(z))
head(rolling.dates)</code></pre>
<pre><code>##      [,1]        [,2]        [,3]        [,4]        [,5]       
## [1,] &quot;1/6/1981&quot;  &quot;1/7/1981&quot;  &quot;1/8/1981&quot;  &quot;1/9/1981&quot;  &quot;1/12/1981&quot;
## [2,] &quot;1/13/1981&quot; &quot;1/14/1981&quot; &quot;1/15/1981&quot; &quot;1/16/1981&quot; &quot;1/19/1981&quot;
## [3,] &quot;1/20/1981&quot; &quot;1/21/1981&quot; &quot;1/22/1981&quot; &quot;1/23/1981&quot; &quot;1/26/1981&quot;
## [4,] &quot;1/27/1981&quot; &quot;1/28/1981&quot; &quot;1/29/1981&quot; &quot;1/30/1981&quot; &quot;2/2/1981&quot; 
## [5,] &quot;2/3/1981&quot;  &quot;2/4/1981&quot;  &quot;2/5/1981&quot;  &quot;2/6/1981&quot;  &quot;2/9/1981&quot; 
## [6,] &quot;2/10/1981&quot; &quot;2/11/1981&quot; &quot;2/13/1981&quot; &quot;2/17/1981&quot; &quot;2/18/1981&quot;
##      [,6]        [,7]        [,8]        [,9]        [,10]      
## [1,] &quot;1/13/1981&quot; &quot;1/14/1981&quot; &quot;1/15/1981&quot; &quot;1/16/1981&quot; &quot;1/19/1981&quot;
## [2,] &quot;1/20/1981&quot; &quot;1/21/1981&quot; &quot;1/22/1981&quot; &quot;1/23/1981&quot; &quot;1/26/1981&quot;
## [3,] &quot;1/27/1981&quot; &quot;1/28/1981&quot; &quot;1/29/1981&quot; &quot;1/30/1981&quot; &quot;2/2/1981&quot; 
## [4,] &quot;2/3/1981&quot;  &quot;2/4/1981&quot;  &quot;2/5/1981&quot;  &quot;2/6/1981&quot;  &quot;2/9/1981&quot; 
## [5,] &quot;2/10/1981&quot; &quot;2/11/1981&quot; &quot;2/13/1981&quot; &quot;2/17/1981&quot; &quot;2/18/1981&quot;
## [6,] &quot;2/19/1981&quot; &quot;2/20/1981&quot; &quot;2/23/1981&quot; &quot;2/24/1981&quot; &quot;2/25/1981&quot;
##      [,11]       [,12]       [,13]       [,14]       [,15]      
## [1,] &quot;1/20/1981&quot; &quot;1/21/1981&quot; &quot;1/22/1981&quot; &quot;1/23/1981&quot; &quot;1/26/1981&quot;
## [2,] &quot;1/27/1981&quot; &quot;1/28/1981&quot; &quot;1/29/1981&quot; &quot;1/30/1981&quot; &quot;2/2/1981&quot; 
## [3,] &quot;2/3/1981&quot;  &quot;2/4/1981&quot;  &quot;2/5/1981&quot;  &quot;2/6/1981&quot;  &quot;2/9/1981&quot; 
## [4,] &quot;2/10/1981&quot; &quot;2/11/1981&quot; &quot;2/13/1981&quot; &quot;2/17/1981&quot; &quot;2/18/1981&quot;
## [5,] &quot;2/19/1981&quot; &quot;2/20/1981&quot; &quot;2/23/1981&quot; &quot;2/24/1981&quot; &quot;2/25/1981&quot;
## [6,] &quot;2/26/1981&quot; &quot;2/27/1981&quot; &quot;3/2/1981&quot;  &quot;3/3/1981&quot;  &quot;3/4/1981&quot; 
##      [,16]       [,17]       [,18]       [,19]       [,20]      
## [1,] &quot;1/27/1981&quot; &quot;1/28/1981&quot; &quot;1/29/1981&quot; &quot;1/30/1981&quot; &quot;2/2/1981&quot; 
## [2,] &quot;2/3/1981&quot;  &quot;2/4/1981&quot;  &quot;2/5/1981&quot;  &quot;2/6/1981&quot;  &quot;2/9/1981&quot; 
## [3,] &quot;2/10/1981&quot; &quot;2/11/1981&quot; &quot;2/13/1981&quot; &quot;2/17/1981&quot; &quot;2/18/1981&quot;
## [4,] &quot;2/19/1981&quot; &quot;2/20/1981&quot; &quot;2/23/1981&quot; &quot;2/24/1981&quot; &quot;2/25/1981&quot;
## [5,] &quot;2/26/1981&quot; &quot;2/27/1981&quot; &quot;3/2/1981&quot;  &quot;3/3/1981&quot;  &quot;3/4/1981&quot; 
## [6,] &quot;3/5/1981&quot;  &quot;3/6/1981&quot;  &quot;3/9/1981&quot;  &quot;3/10/1981&quot; &quot;3/11/1981&quot;</code></pre>
<pre class="r"><code>rownames(rolling.sd)&lt;-rolling.dates[,10]
head(rolling.sd)</code></pre>
<pre><code>##              USGG3M    USGG6M   USGG2YR   USGG3YR   USGG5YR  USGG10YR
## 1/19/1981 0.3433258 0.3262462 0.2748258 0.2030161 0.1713192 0.1299585
## 1/26/1981 0.2933383 0.2907504 0.2261811 0.1499219 0.1450082 0.1146895
## 2/2/1981  0.2613180 0.2437530 0.2006201 0.1632596 0.1654110 0.1459308
## 2/9/1981  0.2551754 0.2469663 0.1989446 0.1692794 0.1717219 0.1551052
## 2/18/1981 0.2480551 0.2481595 0.2102004 0.1786057 0.1744767 0.1643960
## 2/25/1981 0.1963884 0.2363672 0.2095082 0.1809180 0.1822917 0.1664956
##            USGG30YR   Output1
## 1/19/1981 0.1202147 0.5639875
## 1/26/1981 0.1192201 0.4707427
## 2/2/1981  0.1351909 0.4681168
## 2/9/1981  0.1422183 0.4786189
## 2/18/1981 0.1516540 0.4888569
## 2/25/1981 0.1537351 0.4788897</code></pre>
<pre class="r"><code># Plot daily difference rolling standard deviation
matplot(rolling.sd[,c(1,5,7,8)],xaxt=&quot;n&quot;,type=&quot;l&quot;,col=c(&quot;black&quot;,&quot;red&quot;,&quot;blue&quot;,&quot;green&quot;))
axis(side=1,at=1:1656,rownames(rolling.sd))</code></pre>
<p><img src="/posts/2018-01-15-principal-component-analysis-of-financial-data_files/figure-html/unnamed-chunk-41-1.png" width="672" /></p>
<p>Show periods of high volatility. How is volatility related to the level of rates?</p>
<p><strong>Answer:</strong> Volatility is not related to particular values of rates. However we can notice that the periods of high volatility are the periods which are accompanied by the most significant change in rates as it can be expected.</p>
<pre class="r"><code># Show periods of high volatility
high.volatility.periods&lt;-rownames(rolling.sd)[rolling.sd[,8]&gt;.5]
high.volatility.periods</code></pre>
<pre><code>##  [1] &quot;1/19/1981&quot;  &quot;3/25/1981&quot;  &quot;4/1/1981&quot;   &quot;4/8/1981&quot;   &quot;4/23/1981&quot; 
##  [6] &quot;4/30/1981&quot;  &quot;5/7/1981&quot;   &quot;5/14/1981&quot;  &quot;5/21/1981&quot;  &quot;5/29/1981&quot; 
## [11] &quot;6/5/1981&quot;   &quot;6/12/1981&quot;  &quot;6/19/1981&quot;  &quot;6/26/1981&quot;  &quot;7/6/1981&quot;  
## [16] &quot;7/13/1981&quot;  &quot;7/20/1981&quot;  &quot;7/27/1981&quot;  &quot;10/28/1981&quot; &quot;11/5/1981&quot; 
## [21] &quot;11/13/1981&quot; &quot;11/20/1981&quot; &quot;11/30/1981&quot; &quot;12/7/1981&quot;  &quot;12/14/1981&quot;
## [26] &quot;12/29/1981&quot; &quot;1/14/1982&quot;  &quot;1/21/1982&quot;  &quot;1/28/1982&quot;  &quot;2/4/1982&quot;  
## [31] &quot;2/11/1982&quot;  &quot;7/29/1982&quot;  &quot;8/5/1982&quot;   &quot;8/12/1982&quot;  &quot;8/19/1982&quot; 
## [36] &quot;8/26/1982&quot;  &quot;9/24/1982&quot;  &quot;10/1/1982&quot;  &quot;10/8/1982&quot;  &quot;10/18/1982&quot;
## [41] &quot;10/13/1987&quot; &quot;10/20/1987&quot; &quot;10/27/1987&quot; &quot;11/19/2007&quot; &quot;11/26/2007&quot;
## [46] &quot;11/12/2008&quot; &quot;11/19/2008&quot;</code></pre>
<p>Fit linear model to rolling window data using 3 months, 5 years and 30 years variables as predictors.</p>
<pre class="r"><code># Rolling lm coefficients
Coefficients&lt;-rollapply(AssignmentDataRegressionComparison,width=Window.width,by=Window.shift,by.column=FALSE,
        FUN=function(z) coef(lm(Output1~USGG3M+USGG5YR+USGG30YR,data=as.data.frame(z))))
rolling.dates&lt;-rollapply(AssignmentDataRegressionComparison[,1:8],width=Window.width,by=Window.shift,by.column=FALSE, FUN=function(z) rownames(z))
rownames(Coefficients)&lt;-rolling.dates[,10]
Coefficients[1:10,]</code></pre>
<pre><code>##           (Intercept)    USGG3M  USGG5YR  USGG30YR
## 1/16/1981   -15.70877 0.6723609 1.797680 0.2276011
## 1/23/1981   -15.96684 0.6948992 1.480514 0.5529139
## 1/30/1981   -16.77273 0.7078197 1.434388 0.6507280
## 2/6/1981    -16.90734 0.7279033 1.470083 0.6003377
## 2/17/1981   -17.46896 0.7343406 1.361289 0.7499705
## 2/24/1981   -17.04722 0.7357663 1.295641 0.7844908
## 3/3/1981    -17.67901 0.8544681 1.396653 0.5945022
## 3/10/1981   -17.72402 0.9162385 1.654274 0.2571200
## 3/17/1981   -17.00260 0.9265767 1.647852 0.1951273
## 3/24/1981   -16.84302 0.9102780 1.477727 0.3788401</code></pre>
<p>Look at pairwise X-Y plots of regression coefficients for the 3M, 5Yr and 30Yr yields as inputs.</p>
<pre class="r"><code># Pairs plot of Coefficients
pairs(Coefficients)</code></pre>
<p><img src="/posts/2018-01-15-principal-component-analysis-of-financial-data_files/figure-html/unnamed-chunk-44-1.png" width="672" /></p>
<p>Interpret the pairs plots</p>
<p><strong>Answer:</strong> The pairs plot indicate strong negative correlation between the yields of the two long term instruments (5 and 30 year). The relationshup between the 3M bond and the other two is of a different nature: while there are periods when it moves in correlation to the other two rates (slightly negative to 5YR and slightly positive to 30YR coefficients), we can also notice a decoupling effect.</p>
<pre class="r"><code># Plot of coefficients
matplot(Coefficients[,-1],xaxt=&quot;n&quot;,type=&quot;l&quot;,col=c(&quot;black&quot;,&quot;red&quot;,&quot;green&quot;))
axis(side=1,at=1:1657,rownames(Coefficients))</code></pre>
<p><img src="/posts/2018-01-15-principal-component-analysis-of-financial-data_files/figure-html/unnamed-chunk-45-1.png" width="672" /></p>
<pre class="r"><code># Identify high slope periods
high.slopespread.periods&lt;-rownames(Coefficients)[Coefficients[,3]-Coefficients[,4]&gt;3]
jump.slopes&lt;-rownames(Coefficients)[Coefficients[,3]&gt;3]
high.slopespread.periods</code></pre>
<pre><code>##  [1] &quot;4/26/1982&quot;  &quot;12/15/1982&quot; &quot;9/16/1985&quot;  &quot;5/12/1987&quot;  &quot;5/19/1987&quot; 
##  [6] &quot;5/27/1987&quot;  &quot;9/25/1987&quot;  &quot;3/15/1988&quot;  &quot;9/27/1988&quot;  &quot;10/5/1988&quot; 
## [11] &quot;3/10/1989&quot;  &quot;2/5/1992&quot;   &quot;8/3/1994&quot;   &quot;12/8/1994&quot;  &quot;6/14/1996&quot; 
## [16] &quot;5/9/1997&quot;   &quot;5/16/1997&quot;  &quot;5/23/1997&quot;  &quot;5/30/1997&quot;  &quot;12/26/2000&quot;
## [21] &quot;1/2/2001&quot;   &quot;7/25/2001&quot;  &quot;8/1/2001&quot;   &quot;11/13/2003&quot; &quot;8/12/2004&quot; 
## [26] &quot;12/16/2004&quot;</code></pre>
<pre class="r"><code>jump.slopes</code></pre>
<pre><code>## [1] &quot;12/16/2004&quot;</code></pre>
<p>Is the picture of coefficients consistent with the picture of pairs? If yes, explain why.</p>
<p><strong>Answer:</strong> The picture of coefficient is consistent with the pairs picture. The coefficients maintain a clear relationship for most of the times, however there are periods when we see the coefficient merging (especially in the post 2008 era). This disruptions are more noticeable in relation to the black line (3M bond) and the other two lines as the pairs plot indicated.</p>
<p>How often the R-squared is not considered high?</p>
<pre class="r"><code># R-squared
r.squared&lt;-rollapply(AssignmentDataRegressionComparison,width=Window.width,by=Window.shift,by.column=FALSE,
         FUN=function(z) summary(lm(Output1~USGG3M+USGG5YR+USGG30YR,data=as.data.frame(z)))$r.squared)
r.squared&lt;-cbind(rolling.dates[,10],r.squared)
r.squared[1:10,]</code></pre>
<pre><code>##                   r.squared          
##  [1,] &quot;1/16/1981&quot; &quot;0.995046300986446&quot;
##  [2,] &quot;1/23/1981&quot; &quot;0.992485868136646&quot;
##  [3,] &quot;1/30/1981&quot; &quot;0.998641209587999&quot;
##  [4,] &quot;2/6/1981&quot;  &quot;0.998849080081881&quot;
##  [5,] &quot;2/17/1981&quot; &quot;0.997958757207598&quot;
##  [6,] &quot;2/24/1981&quot; &quot;0.996489757136839&quot;
##  [7,] &quot;3/3/1981&quot;  &quot;0.99779753570421&quot; 
##  [8,] &quot;3/10/1981&quot; &quot;0.998963395226792&quot;
##  [9,] &quot;3/17/1981&quot; &quot;0.998729445388789&quot;
## [10,] &quot;3/24/1981&quot; &quot;0.997073000898673&quot;</code></pre>
<pre class="r"><code>plot(r.squared[,2],xaxt=&quot;n&quot;,ylim=c(0,1))
axis(side=1,at=1:1657,rownames(Coefficients))</code></pre>
<p><img src="/posts/2018-01-15-principal-component-analysis-of-financial-data_files/figure-html/unnamed-chunk-46-1.png" width="672" /></p>
<pre class="r"><code>(low.r.squared.periods&lt;-r.squared[r.squared[,2]&lt;.9,1])</code></pre>
<pre><code>## [1] &quot;6/24/1987&quot; &quot;6/27/1991&quot; &quot;4/28/2005&quot; &quot;6/20/2012&quot;</code></pre>
<p>What could cause decrease of <span class="math inline">\(R^2\)</span>?</p>
<p><strong>Answer:</strong> A drop in <span class="math inline">\(R^2\)</span> indicates that the linear model loses the capacity to describe the observed data. This is due to the fact that due to some unusual event in the market, the time series for the yields are decoupling.<br />
In times of severe crises, we might observe an opposite effect altogether. The correlation between the yields might reach values close to 1 and that is due to loss of flexibility in the market. Such effect is noticeable around the year 2008.</p>
<p>Analyze the rolling p-values.</p>
<pre class="r"><code># p-values

Pvalues&lt;-rollapply(AssignmentDataRegressionComparison,width=Window.width,by=Window.shift,by.column=FALSE, FUN=function(z) summary(lm(Output1~USGG3M+USGG5YR+USGG30YR,data=as.data.frame(z)))$coefficients[,4])
rownames(Pvalues)&lt;-rolling.dates[,10]
Pvalues[1:10,]</code></pre>
<pre><code>##            (Intercept)       USGG3M      USGG5YR     USGG30YR
## 1/16/1981 1.193499e-10 3.764585e-10 2.391260e-07 2.538852e-01
## 1/23/1981 3.751077e-12 1.008053e-11 2.447369e-07 5.300949e-03
## 1/30/1981 3.106359e-18 1.406387e-14 4.040035e-09 3.626961e-05
## 2/6/1981  2.591522e-19 3.360104e-19 3.828054e-11 2.221691e-05
## 2/17/1981 1.897239e-16 6.578118e-17 1.461743e-09 1.331767e-04
## 2/24/1981 2.341158e-13 1.000212e-13 9.008221e-07 4.733543e-03
## 3/3/1981  5.435581e-14 1.535503e-11 3.357199e-06 6.010473e-02
## 3/10/1981 6.227624e-16 1.178498e-16 1.679479e-05 3.851840e-01
## 3/17/1981 9.592582e-17 7.065226e-20 1.459692e-05 5.025726e-01
## 3/24/1981 8.248747e-16 6.689840e-16 6.413371e-04 3.052705e-01</code></pre>
<pre class="r"><code>matplot(Pvalues,xaxt=&quot;n&quot;,col=c(&quot;black&quot;,&quot;blue&quot;,&quot;red&quot;,&quot;green&quot;),type=&quot;o&quot;)
axis(side=1,at=1:1657,rownames(Coefficients))</code></pre>
<p><img src="/posts/2018-01-15-principal-component-analysis-of-financial-data_files/figure-html/unnamed-chunk-47-1.png" width="672" /></p>
<pre class="r"><code># Identify high p-values for USGG3M, USGG5YR, USGG30YR
rownames(Pvalues)[Pvalues[,2]&gt;.5]</code></pre>
<pre><code>##  [1] &quot;7/15/1992&quot;  &quot;7/26/1996&quot;  &quot;8/2/1996&quot;   &quot;11/7/2000&quot;  &quot;5/30/2001&quot; 
##  [6] &quot;5/2/2002&quot;   &quot;5/16/2002&quot;  &quot;5/23/2002&quot;  &quot;1/30/2003&quot;  &quot;2/6/2003&quot;  
## [11] &quot;7/24/2003&quot;  &quot;7/31/2003&quot;  &quot;8/7/2003&quot;   &quot;11/20/2003&quot; &quot;12/18/2003&quot;
## [16] &quot;4/28/2005&quot;  &quot;2/10/2006&quot;  &quot;3/9/2007&quot;   &quot;3/16/2007&quot;  &quot;7/21/2009&quot; 
## [21] &quot;10/6/2009&quot;  &quot;10/13/2009&quot; &quot;12/28/2010&quot; &quot;1/11/2011&quot;  &quot;3/1/2011&quot;  
## [26] &quot;11/16/2011&quot; &quot;11/23/2011&quot; &quot;5/23/2012&quot;  &quot;7/11/2012&quot;  &quot;6/6/2013&quot;  
## [31] &quot;1/16/2014&quot;  &quot;1/30/2014&quot;  &quot;3/6/2014&quot;</code></pre>
<pre class="r"><code>rownames(Pvalues)[Pvalues[,3]&gt;.5]</code></pre>
<pre><code>## [1] &quot;12/1/1982&quot; &quot;3/16/1987&quot; &quot;4/28/1987&quot; &quot;6/24/1987&quot; &quot;9/3/1987&quot;  &quot;9/11/1987&quot;
## [7] &quot;9/20/1988&quot; &quot;12/3/1999&quot;</code></pre>
<pre class="r"><code>rownames(Pvalues)[Pvalues[,4]&gt;.5]</code></pre>
<pre><code>##   [1] &quot;3/17/1981&quot;  &quot;4/22/1981&quot;  &quot;4/29/1981&quot;  &quot;6/4/1981&quot;   &quot;10/13/1981&quot;
##   [6] &quot;11/19/1981&quot; &quot;2/3/1982&quot;   &quot;2/26/1982&quot;  &quot;4/2/1982&quot;   &quot;4/12/1982&quot; 
##  [11] &quot;5/3/1982&quot;   &quot;7/7/1982&quot;   &quot;9/1/1982&quot;   &quot;9/23/1982&quot;  &quot;12/8/1982&quot; 
##  [16] &quot;2/18/1983&quot;  &quot;3/1/1983&quot;   &quot;5/4/1983&quot;   &quot;12/30/1983&quot; &quot;1/9/1984&quot;  
##  [21] &quot;2/6/1984&quot;   &quot;3/14/1984&quot;  &quot;3/21/1984&quot;  &quot;4/11/1984&quot;  &quot;6/22/1984&quot; 
##  [26] &quot;6/29/1984&quot;  &quot;10/31/1984&quot; &quot;11/16/1984&quot; &quot;11/26/1984&quot; &quot;12/17/1984&quot;
##  [31] &quot;3/25/1985&quot;  &quot;5/14/1985&quot;  &quot;5/21/1985&quot;  &quot;8/30/1985&quot;  &quot;9/9/1985&quot;  
##  [36] &quot;9/23/1985&quot;  &quot;10/1/1985&quot;  &quot;10/8/1985&quot;  &quot;10/16/1985&quot; &quot;10/23/1985&quot;
##  [41] &quot;10/30/1985&quot; &quot;11/14/1985&quot; &quot;11/21/1985&quot; &quot;1/22/1986&quot;  &quot;1/29/1986&quot; 
##  [46] &quot;5/5/1987&quot;   &quot;12/16/1987&quot; &quot;1/25/1988&quot;  &quot;2/1/1988&quot;   &quot;2/16/1988&quot; 
##  [51] &quot;3/1/1988&quot;   &quot;3/22/1988&quot;  &quot;5/18/1988&quot;  &quot;6/3/1988&quot;   &quot;6/10/1988&quot; 
##  [56] &quot;6/24/1988&quot;  &quot;7/25/1988&quot;  &quot;8/15/1988&quot;  &quot;12/5/1988&quot;  &quot;2/2/1989&quot;  
##  [61] &quot;3/3/1989&quot;   &quot;4/10/1989&quot;  &quot;5/1/1989&quot;   &quot;6/13/1989&quot;  &quot;8/16/1989&quot; 
##  [66] &quot;9/14/1989&quot;  &quot;9/21/1989&quot;  &quot;10/3/1989&quot;  &quot;10/11/1989&quot; &quot;10/18/1989&quot;
##  [71] &quot;11/1/1989&quot;  &quot;11/30/1989&quot; &quot;12/7/1989&quot;  &quot;1/8/1990&quot;   &quot;1/16/1990&quot; 
##  [76] &quot;6/15/1990&quot;  &quot;7/30/1990&quot;  &quot;8/6/1990&quot;   &quot;10/2/1990&quot;  &quot;10/10/1990&quot;
##  [81] &quot;11/23/1990&quot; &quot;3/1/1991&quot;   &quot;5/13/1991&quot;  &quot;5/20/1991&quot;  &quot;6/13/1991&quot; 
##  [86] &quot;7/5/1991&quot;   &quot;7/19/1991&quot;  &quot;9/16/1991&quot;  &quot;2/12/1992&quot;  &quot;2/20/1992&quot; 
##  [91] &quot;3/12/1992&quot;  &quot;4/16/1992&quot;  &quot;4/24/1992&quot;  &quot;5/1/1992&quot;   &quot;5/8/1992&quot;  
##  [96] &quot;6/2/1992&quot;   &quot;6/9/1992&quot;   &quot;6/16/1992&quot;  &quot;8/19/1992&quot;  &quot;8/26/1992&quot; 
## [101] &quot;10/23/1992&quot; &quot;5/20/1993&quot;  &quot;6/11/1993&quot;  &quot;6/18/1993&quot;  &quot;8/30/1993&quot; 
## [106] &quot;12/15/1993&quot; &quot;12/22/1993&quot; &quot;3/16/1994&quot;  &quot;3/30/1994&quot;  &quot;4/6/1994&quot;  
## [111] &quot;4/20/1994&quot;  &quot;4/27/1994&quot;  &quot;6/29/1994&quot;  &quot;8/17/1994&quot;  &quot;9/21/1994&quot; 
## [116] &quot;9/28/1994&quot;  &quot;12/22/1994&quot; &quot;12/29/1994&quot; &quot;1/5/1995&quot;   &quot;1/12/1995&quot; 
## [121] &quot;1/26/1995&quot;  &quot;2/2/1995&quot;   &quot;2/9/1995&quot;   &quot;4/6/1995&quot;   &quot;4/13/1995&quot; 
## [126] &quot;8/25/1995&quot;  &quot;9/29/1995&quot;  &quot;10/27/1995&quot; &quot;11/3/1995&quot;  &quot;11/10/1995&quot;
## [131] &quot;12/29/1995&quot; &quot;1/5/1996&quot;   &quot;1/12/1996&quot;  &quot;1/19/1996&quot;  &quot;3/29/1996&quot; 
## [136] &quot;5/31/1996&quot;  &quot;6/21/1996&quot;  &quot;7/12/1996&quot;  &quot;7/19/1996&quot;  &quot;7/26/1996&quot; 
## [141] &quot;8/2/1996&quot;   &quot;8/9/1996&quot;   &quot;8/16/1996&quot;  &quot;8/23/1996&quot;  &quot;9/27/1996&quot; 
## [146] &quot;10/4/1996&quot;  &quot;12/6/1996&quot;  &quot;2/28/1997&quot;  &quot;3/7/1997&quot;   &quot;4/18/1997&quot; 
## [151] &quot;4/25/1997&quot;  &quot;5/2/1997&quot;   &quot;6/13/1997&quot;  &quot;6/20/1997&quot;  &quot;6/27/1997&quot; 
## [156] &quot;7/4/1997&quot;   &quot;10/10/1997&quot; &quot;10/17/1997&quot; &quot;12/12/1997&quot; &quot;12/19/1997&quot;
## [161] &quot;12/26/1997&quot; &quot;1/9/1998&quot;   &quot;1/16/1998&quot;  &quot;8/14/1998&quot;  &quot;8/21/1998&quot; 
## [166] &quot;8/28/1998&quot;  &quot;9/18/1998&quot;  &quot;9/25/1998&quot;  &quot;12/4/1998&quot;  &quot;12/11/1998&quot;
## [171] &quot;1/8/1999&quot;   &quot;3/12/1999&quot;  &quot;4/2/1999&quot;   &quot;5/14/1999&quot;  &quot;6/25/1999&quot; 
## [176] &quot;7/9/1999&quot;   &quot;7/30/1999&quot;  &quot;8/20/1999&quot;  &quot;9/10/1999&quot;  &quot;9/24/1999&quot; 
## [181] &quot;10/15/1999&quot; &quot;12/31/1999&quot; &quot;3/3/2000&quot;   &quot;3/31/2000&quot;  &quot;4/7/2000&quot;  
## [186] &quot;4/14/2000&quot;  &quot;4/21/2000&quot;  &quot;7/25/2000&quot;  &quot;11/21/2000&quot; &quot;11/28/2000&quot;
## [191] &quot;3/7/2001&quot;   &quot;5/30/2001&quot;  &quot;7/11/2001&quot;  &quot;10/11/2001&quot; &quot;12/13/2001&quot;
## [196] &quot;1/24/2002&quot;  &quot;1/31/2002&quot;  &quot;8/29/2002&quot;  &quot;9/26/2002&quot;  &quot;12/26/2002&quot;
## [201] &quot;1/16/2003&quot;  &quot;1/23/2003&quot;  &quot;3/6/2003&quot;   &quot;3/13/2003&quot;  &quot;3/20/2003&quot; 
## [206] &quot;3/27/2003&quot;  &quot;5/1/2003&quot;   &quot;6/19/2003&quot;  &quot;6/26/2003&quot;  &quot;7/3/2003&quot;  
## [211] &quot;9/18/2003&quot;  &quot;9/25/2003&quot;  &quot;10/16/2003&quot; &quot;10/23/2003&quot; &quot;10/30/2003&quot;
## [216] &quot;11/20/2003&quot; &quot;1/1/2004&quot;   &quot;2/5/2004&quot;   &quot;2/26/2004&quot;  &quot;3/4/2004&quot;  
## [221] &quot;4/15/2004&quot;  &quot;4/22/2004&quot;  &quot;5/13/2004&quot;  &quot;5/27/2004&quot;  &quot;6/3/2004&quot;  
## [226] &quot;6/17/2004&quot;  &quot;6/24/2004&quot;  &quot;7/8/2004&quot;   &quot;10/14/2004&quot; &quot;10/21/2004&quot;
## [231] &quot;10/28/2004&quot; &quot;11/18/2004&quot; &quot;12/23/2004&quot; &quot;3/17/2005&quot;  &quot;3/24/2005&quot; 
## [236] &quot;3/31/2005&quot;  &quot;4/7/2005&quot;   &quot;7/21/2005&quot;  &quot;8/11/2005&quot;  &quot;9/22/2005&quot; 
## [241] &quot;10/6/2005&quot;  &quot;11/3/2005&quot;  &quot;12/8/2005&quot;  &quot;12/15/2005&quot; &quot;1/20/2006&quot; 
## [246] &quot;5/12/2006&quot;  &quot;5/19/2006&quot;  &quot;5/26/2006&quot;  &quot;6/2/2006&quot;   &quot;6/9/2006&quot;  
## [251] &quot;6/16/2006&quot;  &quot;7/7/2006&quot;   &quot;7/21/2006&quot;  &quot;10/6/2006&quot;  &quot;11/3/2006&quot; 
## [256] &quot;1/12/2007&quot;  &quot;2/23/2007&quot;  &quot;3/16/2007&quot;  &quot;4/27/2007&quot;  &quot;5/25/2007&quot; 
## [261] &quot;9/7/2007&quot;   &quot;9/14/2007&quot;  &quot;9/21/2007&quot;  &quot;9/28/2007&quot;  &quot;11/16/2007&quot;
## [266] &quot;5/5/2009&quot;   &quot;6/1/2010&quot;   &quot;6/15/2010&quot;  &quot;1/25/2011&quot;  &quot;2/1/2011&quot;  
## [271] &quot;6/12/2014&quot;</code></pre>
<p>Interpret the plot <strong>Answer:</strong> In this case we are dealing with p-values which serve as indicators of a process rather than just characteristics of a simple linear model.<br />
They help identify various periods when one set of predictors is more or less important than the others. We can see that throughout the recorded period, the 3M and especially 5Y bond serve as the most significant predictors of the output variable. However there are moments when this rule is broken. During 1987, we can see that the 5YR bond becomes an insignificant predictor and similarly after the crisis of 2008, this happens to the 3M bond. This might be due to the phenomenon called “flight to safety” which occurs in periods of severe financial crisis, and is characterised by a sharp increase in demand for these type of securities.</p>
</div>
<div id="step-7" class="section level1">
<h1>Step 7</h1>
<p>Perform PCA with the inputs (columns 1-7).</p>
<pre class="r"><code>AssignmentData.Output&lt;-AssignmentData$Output1
AssignmentData&lt;-data.matrix(AssignmentData[,1:7],rownames.force=&quot;automatic&quot;)
dim(AssignmentData)</code></pre>
<pre><code>## [1] 8300    7</code></pre>
<pre class="r"><code>head(AssignmentData)</code></pre>
<pre><code>##           USGG3M USGG6M USGG2YR USGG3YR USGG5YR USGG10YR USGG30YR
## 1/5/1981   13.52  13.09  12.289   12.28  12.294   12.152   11.672
## 1/6/1981   13.58  13.16  12.429   12.31  12.214   12.112   11.672
## 1/7/1981   14.50  13.90  12.929   12.78  12.614   12.382   11.892
## 1/8/1981   14.76  14.00  13.099   12.95  12.684   12.352   11.912
## 1/9/1981   15.20  14.30  13.539   13.28  12.884   12.572   12.132
## 1/12/1981  15.22  14.23  13.179   12.94  12.714   12.452   12.082</code></pre>
<p>Explore the dimensionality of the set of 3M, 2Y and 5Y yields.</p>
<pre class="r"><code># Select 3 variables. Explore dimensionality and correlation 
AssignmentData.3M_2Y_5Y&lt;-AssignmentData[,c(1,3,5)]
pairs(AssignmentData.3M_2Y_5Y)</code></pre>
<p><img src="/posts/2018-01-15-principal-component-analysis-of-financial-data_files/figure-html/unnamed-chunk-49-1.png" width="672" /></p>
<p>Observe the 3D plot of the set. Use library rgl:<br />
<code>rgl</code> seems to have problems with the OSX. Using plotly instead.</p>
<pre class="r"><code>#library(plotly)
#p &lt;- plot_ly(data.frame(AssignmentData.3M_2Y_5Y), x = ~USGG3M, y = ~USGG2YR, z = ~USGG5YR) %&gt;%
#  add_markers() %&gt;%
#  layout(scene = list(xaxis = list(title = &#39;USGG3M&#39;),
#                     yaxis = list(title = &#39;USGG2YR&#39;),
#                     zaxis = list(title = &#39;USGG5YR&#39;)))
#p</code></pre>
<p>Analyze the covariance matrix of the data. Compare results of manual calculation and <code>cov()</code>.</p>
<pre class="r"><code># Calculate manually
# Note that Y.0 = Y - L.0
Y.0 &lt;- apply(X = AssignmentData, MARGIN = 2,FUN = function(x) x-mean(x))
N &lt;- nrow(AssignmentData)
Manual.Covariance.Matrix &lt;- (t(Y.0)%*%Y.0)/(N-1)
# Using cov() function
Covariance.Matrix &lt;- cov(AssignmentData)

# Print results
Manual.Covariance.Matrix</code></pre>
<pre><code>##             USGG3M    USGG6M   USGG2YR   USGG3YR   USGG5YR  USGG10YR
## USGG3M   11.760393 11.855287 12.303031 11.942035 11.188856  9.924865
## USGG6M   11.855287 12.000510 12.512434 12.158422 11.406959 10.128890
## USGG2YR  12.303031 12.512434 13.284203 12.977542 12.279514 11.005377
## USGG3YR  11.942035 12.158422 12.977542 12.708647 12.068078 10.856033
## USGG5YR  11.188856 11.406959 12.279514 12.068078 11.543082 10.463386
## USGG10YR  9.924865 10.128890 11.005377 10.856033 10.463386  9.583483
## USGG30YR  8.587987  8.768702  9.600181  9.497246  9.212159  8.510632
##          USGG30YR
## USGG3M   8.587987
## USGG6M   8.768702
## USGG2YR  9.600181
## USGG3YR  9.497246
## USGG5YR  9.212159
## USGG10YR 8.510632
## USGG30YR 7.624304</code></pre>
<pre class="r"><code>Covariance.Matrix</code></pre>
<pre><code>##             USGG3M    USGG6M   USGG2YR   USGG3YR   USGG5YR  USGG10YR
## USGG3M   11.760393 11.855287 12.303031 11.942035 11.188856  9.924865
## USGG6M   11.855287 12.000510 12.512434 12.158422 11.406959 10.128890
## USGG2YR  12.303031 12.512434 13.284203 12.977542 12.279514 11.005377
## USGG3YR  11.942035 12.158422 12.977542 12.708647 12.068078 10.856033
## USGG5YR  11.188856 11.406959 12.279514 12.068078 11.543082 10.463386
## USGG10YR  9.924865 10.128890 11.005377 10.856033 10.463386  9.583483
## USGG30YR  8.587987  8.768702  9.600181  9.497246  9.212159  8.510632
##          USGG30YR
## USGG3M   8.587987
## USGG6M   8.768702
## USGG2YR  9.600181
## USGG3YR  9.497246
## USGG5YR  9.212159
## USGG10YR 8.510632
## USGG30YR 7.624304</code></pre>
<p>Plot the covariance matrix.</p>
<pre class="r"><code>Maturities&lt;-c(.25,.5,2,3,5,10,30)
contour(Maturities,Maturities,Covariance.Matrix)</code></pre>
<p><img src="/posts/2018-01-15-principal-component-analysis-of-financial-data_files/figure-html/unnamed-chunk-52-1.png" width="672" /></p>
<p>Perform the PCA by manually calculating factors, loadings and analyzing the importance of factors.</p>
<p>Find eigenvalues and eigenvectors. Calculate vector of means (zero loading), first 3 loadings and 3 factors.</p>
<pre class="r"><code># Calculate eigenvalues and eigenvectors
Eigen.Decomposition &lt;- eigen(Covariance.Matrix)

# Vector of means
L.0 &lt;- apply(AssignmentData,2,mean)

# First 3 loading vectors
Loadings &lt;- Eigen.Decomposition$vectors[,1:3]

# First 3 factor scores
Factors &lt;- Y.0%*%Loadings</code></pre>
<p>See importance of factors</p>
<pre class="r"><code>barplot(Eigen.Decomposition$values/sum(Eigen.Decomposition$values),width=2,col = &quot;black&quot;, names.arg=c(&quot;F1&quot;,&quot;F2&quot;,&quot;F3&quot;,&quot;F4&quot;,&quot;F5&quot;,&quot;F6&quot;,&quot;F7&quot;))</code></pre>
<p><img src="/posts/2018-01-15-principal-component-analysis-of-financial-data_files/figure-html/unnamed-chunk-54-1.png" width="672" /></p>
<p>Plot the loadings</p>
<pre class="r"><code>matplot(Maturities,Loadings,type=&quot;l&quot;,lty=1,col=c(&quot;black&quot;,&quot;red&quot;,&quot;green&quot;),lwd=3)</code></pre>
<p><img src="/posts/2018-01-15-principal-component-analysis-of-financial-data_files/figure-html/unnamed-chunk-55-1.png" width="672" /></p>
<p>Interpret the factors by looking at the shapes of the loadings.<br />
<strong>Answer:</strong> The interaction of factors and loadings in predicting the response is described by <span class="math display">\[ \hat Y_0 = \hat F \hat L^T \]</span></p>
<p>The effect of <strong>increase</strong> in each of the three factors will lead to:</p>
<ul>
<li>Factor 1 is associated with loading 1 and will produce a downward shift in all the yields for all maturities<br />
</li>
<li>Factor 2 is assocatied with loading 2 and will cause a twist or tilt effect by causing a downward shift on the lower maturity yields and an upwards shift on the longer maturity yields (counterclockwise rotation).</li>
<li>Factor 3 is associated with loading 3 and will cause a butterfly effect by causing a dip in the mid to low range maturitues and an upwards shift on the two extremes.</li>
</ul>
<p>Calculate and plot 3 selected factors</p>
<pre class="r"><code>matplot(Factors,type=&quot;l&quot;,col=c(&quot;black&quot;,&quot;red&quot;,&quot;green&quot;),lty=1,lwd=3)</code></pre>
<p><img src="/posts/2018-01-15-principal-component-analysis-of-financial-data_files/figure-html/unnamed-chunk-56-1.png" width="672" /></p>
<p>Change the signs of the first factor and the corresponding factor loading.</p>
<pre class="r"><code>Loadings[,1]&lt;- -Loadings[,1]
Factors[,1]&lt;- -Factors[,1]
matplot(Factors,type=&quot;l&quot;,col=c(&quot;black&quot;,&quot;red&quot;,&quot;green&quot;),lty=1,lwd=3)</code></pre>
<p><img src="/posts/2018-01-15-principal-component-analysis-of-financial-data_files/figure-html/unnamed-chunk-57-1.png" width="672" /></p>
<pre class="r"><code>matplot(Maturities,Loadings,type=&quot;l&quot;,lty=1,col=c(&quot;black&quot;,&quot;red&quot;,&quot;green&quot;),lwd=3)</code></pre>
<p><img src="/posts/2018-01-15-principal-component-analysis-of-financial-data_files/figure-html/unnamed-chunk-57-2.png" width="672" /></p>
<pre class="r"><code>plot(Factors[,1],Factors[,2],type=&quot;l&quot;,lwd=2)</code></pre>
<p><img src="/posts/2018-01-15-principal-component-analysis-of-financial-data_files/figure-html/unnamed-chunk-58-1.png" width="672" /></p>
<p>Draw at least three conclusions from the plot of the first two factors above.</p>
<p><strong>Answer:</strong><br />
1. The correlation between factors 1 and 2 has an economic rationale behind it. In the selected period we have a positive correlation between Factor 1 and Factor 2. In a high rate environment such as the one shown below (around data point 2000, or 1988 through 1989 period, marked in orange dots), Factor 1 goes down, and together with it Factor 2 goes up in this period (while remaining negative). This will lead to a general downshift in rates caused by the shape of loading vector 1. However this effect is amplified in long end rates by the shape of loading vector 2.<br />
In a high rate environment, the FED pulls the rates down by pulling the short end rates down. The long end rates are controlled by mortgage-lending banks (the market). The interaction between these two forces results in the market over reacting to the FEDs decisions, and that is why we see an amplified effect in the long end rates (i.e long end rates going even further down.)</p>
<pre class="r"><code># Period under consideration
rownames(AssignmentData[1900:2000,])[c(1,100)]</code></pre>
<pre><code>## [1] &quot;9/20/1988&quot; &quot;2/15/1989&quot;</code></pre>
<pre class="r"><code>plot(Factors[,1],Factors[,2],type=&quot;l&quot;,lwd=2)
points(Factors[1900:2000,1],Factors[1900:2000,2], col=&quot;orange&quot;)</code></pre>
<p><img src="/posts/2018-01-15-principal-component-analysis-of-financial-data_files/figure-html/unnamed-chunk-59-1.png" width="672" /></p>
<pre class="r"><code>matplot(Maturities,Loadings,type=&quot;l&quot;,lty=1,col=c(&quot;black&quot;,&quot;red&quot;,&quot;green&quot;),lwd=3)</code></pre>
<p><img src="/posts/2018-01-15-principal-component-analysis-of-financial-data_files/figure-html/unnamed-chunk-59-2.png" width="672" /></p>
<pre class="r"><code>matplot(AssignmentData[,-c(8,9,10)],type=&#39;l&#39;)</code></pre>
<p><img src="/posts/2018-01-15-principal-component-analysis-of-financial-data_files/figure-html/unnamed-chunk-59-3.png" width="672" /></p>
<ol start="2" style="list-style-type: decimal">
<li>We can notice in the plot below that the factors are changing significantly from day to day in the first 100 recroded days (marked in red dots). This indicates that this period was marked by high volatility. This corresponds to early 1981.<br />
As a comparison we can notice the difference with a selection of 100 days in the firs half of the year 1985. The factors scores in this period are much more crunched together and change less from day to day. This indicates a period of low volatility. An historical event which might describe this pattern is what is know as the Great Moderation, a period of reduction in volatiliy of business cycle fluctuations which started in this exact period.</li>
</ol>
<pre class="r"><code># Start and End Dates for Period 1
rownames(AssignmentData[1:100,])[c(1,100)]</code></pre>
<pre><code>## [1] &quot;1/5/1981&quot;  &quot;5/28/1981&quot;</code></pre>
<pre class="r"><code>plot(Factors[,1],Factors[,2],type=&quot;l&quot;,lwd=2)
points(Factors[1:100,1],Factors[1:100,2], col=&quot;red&quot;)

# Start and End Dates for Period 2
rownames(AssignmentData[1000:1100,])[c(1,100)]</code></pre>
<pre><code>## [1] &quot;1/25/1985&quot; &quot;7/3/1985&quot;</code></pre>
<pre class="r"><code>points(Factors[1000:1100,1],Factors[1000:1100,2], col=&quot;blue&quot;)</code></pre>
<p><img src="/posts/2018-01-15-principal-component-analysis-of-financial-data_files/figure-html/unnamed-chunk-60-1.png" width="672" /></p>
<ol start="3" style="list-style-type: decimal">
<li>The highlighted period represents the months leading to and immediately following the Black Monday crash on 1987. We can see that the after crash period was characterizied by very low volatility in rates, its one of the most dense areas in the entire plot.</li>
</ol>
<pre class="r"><code>plot(Factors[,1],Factors[,2],type=&quot;l&quot;,lwd=2)
points(Factors[1650:1750,1],Factors[1650:1750,2], col=&quot;green&quot;)
points(Factors[1750:1850,1],Factors[1750:1850,2], col=&quot;red&quot;)</code></pre>
<p><img src="/posts/2018-01-15-principal-component-analysis-of-financial-data_files/figure-html/unnamed-chunk-61-1.png" width="672" /></p>
<pre class="r"><code>rownames(AssignmentData[1650:1850,])[c(1,200)]</code></pre>
<pre><code>## [1] &quot;9/18/1987&quot; &quot;7/8/1988&quot;</code></pre>
<p>Analyze the adjustments that each factor makes to the term curve.</p>
<pre class="r"><code>OldCurve&lt;-AssignmentData[135,]
NewCurve&lt;-AssignmentData[136,]
CurveChange&lt;-NewCurve-OldCurve
FactorsChange&lt;-Factors[136,]-Factors[135,]
ModelCurveAdjustment.1Factor&lt;-OldCurve+t(Loadings[,1])*FactorsChange[1]
ModelCurveAdjustment.2Factors&lt;-OldCurve+t(Loadings[,1])*FactorsChange[1]+t(Loadings[,2])*FactorsChange[2]
ModelCurveAdjustment.3Factors&lt;-OldCurve+t(Loadings[,1])*FactorsChange[1]+t(Loadings[,2])*FactorsChange[2]+
  t(Loadings[,3])*FactorsChange[3]
matplot(Maturities,
        t(rbind(OldCurve,NewCurve,ModelCurveAdjustment.1Factor,ModelCurveAdjustment.2Factors,
                ModelCurveAdjustment.3Factors)),
        type=&quot;l&quot;,lty=c(1,1,2,2,2),col=c(&quot;black&quot;,&quot;red&quot;,&quot;green&quot;,&quot;blue&quot;,&quot;magenta&quot;),lwd=3,ylab=&quot;Curve Adjustment&quot;)
legend(x=&quot;topright&quot;,c(&quot;Old Curve&quot;,&quot;New Curve&quot;,&quot;1-Factor Adj.&quot;,&quot;2-Factor Adj.&quot;,
                      &quot;3-Factor Adj.&quot;),lty=c(1,1,2,2,2),lwd=3,col=c(&quot;black&quot;,&quot;red&quot;,&quot;green&quot;,&quot;blue&quot;,&quot;magenta&quot;))</code></pre>
<p><img src="/posts/2018-01-15-principal-component-analysis-of-financial-data_files/figure-html/unnamed-chunk-62-1.png" width="672" /></p>
<p>Explain how shapes of the loadings affect the adjustments using only factor 1, factors 1 and 2, and all 3 factors.</p>
<p><strong>Answer:</strong> Factor 1 is associated with loading shape 1 which causes just an upwards uniform shift (the green line). Factor 2 is causing the twisting or clockwise rotating effect shown by the blue line. Factor 3 cuases the butterfly effect by affecting the central portion and the extremes in oppposite ways</p>
<pre class="r"><code>FactorsChange</code></pre>
<pre><code>## [1]  2.13018277 -0.45938824  0.07368842</code></pre>
<pre class="r"><code>Loadings</code></pre>
<pre><code>##           [,1]        [,2]       [,3]
## [1,] 0.3839609 -0.50744508  0.5298222
## [2,] 0.3901870 -0.43946144  0.1114737
## [3,] 0.4151851 -0.11112721 -0.4187873
## [4,] 0.4063541  0.01696988 -0.4476561
## [5,] 0.3860610  0.23140317 -0.2462364
## [6,] 0.3477544  0.43245979  0.1500903
## [7,] 0.3047124  0.54421228  0.4979195</code></pre>
<p>See the goodness of fit for the example of 10Y yield.</p>
<pre class="r"><code># How close is the approximation for each maturity?

cbind(Maturities,Loadings)</code></pre>
<pre><code>##      Maturities                                 
## [1,]       0.25 0.3839609 -0.50744508  0.5298222
## [2,]       0.50 0.3901870 -0.43946144  0.1114737
## [3,]       2.00 0.4151851 -0.11112721 -0.4187873
## [4,]       3.00 0.4063541  0.01696988 -0.4476561
## [5,]       5.00 0.3860610  0.23140317 -0.2462364
## [6,]      10.00 0.3477544  0.43245979  0.1500903
## [7,]      30.00 0.3047124  0.54421228  0.4979195</code></pre>
<pre class="r"><code>Model.10Y&lt;-L.0[6]+Loadings[6,1]*Factors[,1]+Loadings[6,2]*Factors[,2]+Loadings[6,3]*Factors[,3]
matplot(cbind(AssignmentData[,6],Model.10Y),type=&quot;l&quot;,lty=1,lwd=c(3,1),col=c(&quot;black&quot;,&quot;red&quot;),ylab=&quot;5Y Yield&quot;)</code></pre>
<p><img src="/posts/2018-01-15-principal-component-analysis-of-financial-data_files/figure-html/unnamed-chunk-64-1.png" width="672" /></p>
<p>Repeat the PCA using princomp</p>
<pre class="r"><code># Do PCA analysis using princomp()
PCA.Yields&lt;-princomp(AssignmentData)
names(PCA.Yields)</code></pre>
<pre><code>## [1] &quot;sdev&quot;     &quot;loadings&quot; &quot;center&quot;   &quot;scale&quot;    &quot;n.obs&quot;    &quot;scores&quot;  
## [7] &quot;call&quot;</code></pre>
<p>Compare the loadings.</p>
<pre class="r"><code># Check that the loadings are the same
cbind(PCA.Yields$loadings[,1:3],Maturities,Eigen.Decomposition$vectors[,1:3])</code></pre>
<pre><code>##              Comp.1      Comp.2     Comp.3 Maturities           
## USGG3M   -0.3839609  0.50744508  0.5298222       0.25 -0.3839609
## USGG6M   -0.3901870  0.43946144  0.1114737       0.50 -0.3901870
## USGG2YR  -0.4151851  0.11112721 -0.4187873       2.00 -0.4151851
## USGG3YR  -0.4063541 -0.01696988 -0.4476561       3.00 -0.4063541
## USGG5YR  -0.3860610 -0.23140317 -0.2462364       5.00 -0.3860610
## USGG10YR -0.3477544 -0.43245979  0.1500903      10.00 -0.3477544
## USGG30YR -0.3047124 -0.54421228  0.4979195      30.00 -0.3047124
##                                
## USGG3M   -0.50744508  0.5298222
## USGG6M   -0.43946144  0.1114737
## USGG2YR  -0.11112721 -0.4187873
## USGG3YR   0.01696988 -0.4476561
## USGG5YR   0.23140317 -0.2462364
## USGG10YR  0.43245979  0.1500903
## USGG30YR  0.54421228  0.4979195</code></pre>
<pre class="r"><code>matplot(Maturities,PCA.Yields$loadings[,1:3],type=&quot;l&quot;,col=c(&quot;black&quot;,&quot;red&quot;,&quot;green&quot;),lty=1,lwd=3)</code></pre>
<p><img src="/posts/2018-01-15-principal-component-analysis-of-financial-data_files/figure-html/unnamed-chunk-66-1.png" width="672" /></p>
<pre class="r"><code>matplot(PCA.Yields$scores[,1:3],type=&quot;l&quot;,col=c(&quot;black&quot;,&quot;red&quot;,&quot;green&quot;),lwd=3,lty=1)</code></pre>
<p><img src="/posts/2018-01-15-principal-component-analysis-of-financial-data_files/figure-html/unnamed-chunk-66-2.png" width="672" /></p>
<p>Change the signs of the first factor and factor loading again.</p>
<pre class="r"><code># Change the signs of the 1st factor and the first loading
PCA.Yields$loadings[,1]&lt;--PCA.Yields$loadings[,1]
PCA.Yields$scores[,1]&lt;--PCA.Yields$scores[,1]
matplot(Maturities,PCA.Yields$loadings[,1:3],type=&quot;l&quot;,col=c(&quot;black&quot;,&quot;red&quot;,&quot;green&quot;),lty=1,lwd=3)</code></pre>
<p><img src="/posts/2018-01-15-principal-component-analysis-of-financial-data_files/figure-html/unnamed-chunk-67-1.png" width="672" /></p>
<pre class="r"><code>matplot(PCA.Yields$scores[,1:3],type=&quot;l&quot;,col=c(&quot;black&quot;,&quot;red&quot;,&quot;green&quot;),lwd=3,lty=1)</code></pre>
<p><img src="/posts/2018-01-15-principal-component-analysis-of-financial-data_files/figure-html/unnamed-chunk-67-2.png" width="672" /></p>
<p>Uncover the mystery of the Output in column 8.</p>
<pre class="r"><code># What variable we had as Output?
matplot(cbind(PCA.Yields$scores[,1],AssignmentData.Output,Factors[,1]),type=&quot;l&quot;,col=c(&quot;black&quot;,&quot;red&quot;,&quot;green&quot;),lwd=c(3,2,1),lty=c(1,2,3),ylab=&quot;Factor 1&quot;)</code></pre>
<p><img src="/posts/2018-01-15-principal-component-analysis-of-financial-data_files/figure-html/unnamed-chunk-68-1.png" width="672" /></p>
<p>Compare the regression coefficients from Step 2 and Step 3 with factor loadings.<br />
First, look at the slopes for <code>AssignmentData.Input~AssignmentData.Output</code>.</p>
<pre class="r"><code>t(apply(AssignmentData, 2, function(AssignmentData.col) lm(AssignmentData.col~AssignmentData.Output)$coef))</code></pre>
<pre><code>##          (Intercept) AssignmentData.Output
## USGG3M      4.675134             0.3839609
## USGG6M      4.844370             0.3901870
## USGG2YR     5.438888             0.4151851
## USGG3YR     5.644458             0.4063541
## USGG5YR     6.009421             0.3860610
## USGG10YR    6.481316             0.3477544
## USGG30YR    6.869355             0.3047124</code></pre>
<pre class="r"><code>cbind(PCA.Yields$center,PCA.Yields$loadings[,1])</code></pre>
<pre><code>##              [,1]      [,2]
## USGG3M   4.675134 0.3839609
## USGG6M   4.844370 0.3901870
## USGG2YR  5.438888 0.4151851
## USGG3YR  5.644458 0.4063541
## USGG5YR  6.009421 0.3860610
## USGG10YR 6.481316 0.3477544
## USGG30YR 6.869355 0.3047124</code></pre>
<p>This shows that the zero loading equals the vector of intercepts of models Y~Output1, where Y is one of the columns of yields in the data. Also, the slopes of the same models are equal to the first loading.</p>
<p>Check if the same is true in the opposite direction: is there a correspondence between the coefficients of models Output1~Yield and the first loading.</p>
<pre class="r"><code>AssignmentData.Centered&lt;-t(apply(AssignmentData,1,function(AssignmentData.row) AssignmentData.row-PCA.Yields$center))
dim(AssignmentData.Centered)</code></pre>
<pre><code>## [1] 8300    7</code></pre>
<pre class="r"><code>t(apply(AssignmentData.Centered, 2, function(AssignmentData.col) lm(AssignmentData.Output~AssignmentData.col)$coef))</code></pre>
<pre><code>##           (Intercept) AssignmentData.col
## USGG3M   1.420077e-11           2.507561
## USGG6M   1.421187e-11           2.497235
## USGG2YR  1.419747e-11           2.400449
## USGG3YR  1.419989e-11           2.455793
## USGG5YR  1.419549e-11           2.568742
## USGG10YR 1.420297e-11           2.786991
## USGG30YR 1.420965e-11           3.069561</code></pre>
<p>To recover the loading of the first factor by doing regression, use all inputs together.</p>
<pre class="r"><code>t(lm(AssignmentData.Output~AssignmentData.Centered)$coef)[-1]</code></pre>
<pre><code>## [1] 0.3839609 0.3901870 0.4151851 0.4063541 0.3860610 0.3477544 0.3047124</code></pre>
<pre class="r"><code>PCA.Yields$loadings[,1]</code></pre>
<pre><code>##    USGG3M    USGG6M   USGG2YR   USGG3YR   USGG5YR  USGG10YR  USGG30YR 
## 0.3839609 0.3901870 0.4151851 0.4063541 0.3860610 0.3477544 0.3047124</code></pre>
<p>This means that the factor is a portfolio of all input variables with weights.</p>
<pre class="r"><code>PCA.Yields$loadings[,1]</code></pre>
<pre><code>##    USGG3M    USGG6M   USGG2YR   USGG3YR   USGG5YR  USGG10YR  USGG30YR 
## 0.3839609 0.3901870 0.4151851 0.4063541 0.3860610 0.3477544 0.3047124</code></pre>
</div>
